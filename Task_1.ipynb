{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2ddd2c",
   "metadata": {},
   "outputs": [],
   "source": [
   "# è«‹å¾é€™è£¡ä¸‹è¼‰ä½œæ¥­ç”¨è³‡æ–™\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://cchsu.info/files/images.zip\"\n",
    "output_path = \"images.zip\"\n",
    "\n",
    "urllib.request.urlretrieve(url, output_path)\n",
    "print(\"ä¸‹è¼‰å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3593f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "11.8\n",
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))  # å¦‚æœæœ‰ GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c74dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e991b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ–ç‰‡è®€å–\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class TxtImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    å¾ txt æª”è®€å–åœ–ç‰‡è·¯å¾‘èˆ‡é¡åˆ¥ï¼Œæ ¼å¼ç‚ºï¼š\n",
    "    /path/to/image1.jpg 0\n",
    "    /path/to/image2.jpg 1\n",
    "    \"\"\"\n",
    "    def __init__(self, txt_file, transform=None):\n",
    "        self.samples = []\n",
    "        with open(txt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                path, label = line.strip().split()\n",
    "                self.samples.append((path, int(label)))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.samples[index]\n",
    "        image = Image.open(img_path).convert('RGB')  # å¼·åˆ¶ç‚º RGB æ ¼å¼\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8532f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Convolution\n",
    "\n",
    "class SimpleInception(nn.Module):\n",
    "    \"\"\"\n",
    "    ç°¡åŒ–ç‰ˆçš„ Inception æ¨¡çµ„ï¼Œç”¨æ–¼æ¯å€‹é€šé“ç¨ç«‹è™•ç†ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(SimpleInception, self).__init__()\n",
    "        self.branch1 = nn.Conv2d(in_channels, 8, kernel_size=1)\n",
    "        self.branch3 = nn.Conv2d(in_channels, 8, kernel_size=3, padding=1)\n",
    "        self.branch5 = nn.Conv2d(in_channels, 8, kernel_size=5, padding=2)\n",
    "        self.pool_proj = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, 8, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b3 = self.branch3(x)\n",
    "        b5 = self.branch5(x)\n",
    "        bp = self.pool_proj(x)\n",
    "        return torch.cat([b1, b3, b5, bp], dim=1)  # (B, 32, H, W)\n",
    "\n",
    "class ChannelWiseInceptionAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    ä¸»çµæ§‹ï¼šæ¯é€šé“è·‘ç°¡ç‰ˆ Inception -> concat -> Adaptive Pool -> Conv1x1\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size=(224, 224)):\n",
    "        super(ChannelWiseInceptionAggregator, self).__init__()\n",
    "        self.inception_modules = nn.ModuleList([SimpleInception(1) for _ in range(3)])  # æœ€å¤šæ”¯æ´ RGB\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size)\n",
    "        self.fusion = nn.Conv2d(32 * 3, 3, kernel_size=1)  # ä¸‰é€šé“å„æœ‰ 32 å€‹é€šé“ç‰¹å¾µ\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C, H, W)\n",
    "        B, C, H, W = x.size()\n",
    "        features = []\n",
    "        for c in range(C):\n",
    "            xi = x[:, c:c+1, :, :]  # å–å–®é€šé“\n",
    "            fi = self.inception_modules[c](xi)  # (B, 32, H, W)\n",
    "            features.append(fi)\n",
    "\n",
    "        x = torch.cat(features, dim=1)  # (B, 32*C, H, W)\n",
    "        x = self.pool(x)               # (B, 32*C, 224, 224)\n",
    "        x = self.fusion(x)            # (B, 3, 224, 224)\n",
    "        return x  # å°‡è™•ç†å®Œçš„ feature map å›å‚³çµ¦å¤–éƒ¨ (é€é€² AlexNet ç­‰åˆ†é¡å™¨)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5eef302",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TxtImageDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# å°ç…§çµ„\u001b[39;00m\n\u001b[32m      6\u001b[39m transform_control = T.Compose([\n\u001b[32m      7\u001b[39m     T.Resize((\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)),  \u001b[38;5;66;03m# Resize åœ–ç‰‡åˆ° 224x224ï¼ŒAlexNet çš„è¼¸å…¥å¤§å°\u001b[39;00m\n\u001b[32m      8\u001b[39m     T.ToTensor()\n\u001b[32m      9\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m train_control_dataset = \u001b[43mTxtImageDataset\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mtrain.txt\u001b[39m\u001b[33m'\u001b[39m, transform=transform_control)\n\u001b[32m     12\u001b[39m val_control_dataset = TxtImageDataset(\u001b[33m'\u001b[39m\u001b[33mval.txt\u001b[39m\u001b[33m'\u001b[39m, transform=transform_control)\n\u001b[32m     13\u001b[39m test_control_dataset = TxtImageDataset(\u001b[33m'\u001b[39m\u001b[33mtest.txt\u001b[39m\u001b[33m'\u001b[39m, transform=transform_control)\n",
      "\u001b[31mNameError\u001b[39m: name 'TxtImageDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# åœ–ç‰‡è™•ç†\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# å°ç…§çµ„\n",
    "transform_control = T.Compose([\n",
    "    T.Resize((224, 224)),  # Resize åœ–ç‰‡åˆ° 224x224ï¼ŒAlexNet çš„è¼¸å…¥å¤§å°\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_control_dataset = TxtImageDataset('train.txt', transform=transform_control)\n",
    "val_control_dataset = TxtImageDataset('val.txt', transform=transform_control)\n",
    "test_control_dataset = TxtImageDataset('test.txt', transform=transform_control)\n",
    "\n",
    "# è®€å–è³‡æ–™é›†\n",
    "batch_size = 32\n",
    "train_control_loader = DataLoader(train_control_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_control_loader = DataLoader(val_control_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_control_loader = DataLoader(test_control_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d779e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦é©—çµ„å°ˆç”¨DataLoader\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "class BatchListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, batches, transform=None):\n",
    "        self.batches = batches\n",
    "        self.transform = transform or T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.batches[idx]\n",
    "        images, labels = [], []\n",
    "        max_h, max_w = 0, 0\n",
    "\n",
    "        for path, label in batch:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            tensor = self.transform(img)\n",
    "            images.append(tensor)\n",
    "            labels.append(label)\n",
    "            max_h = max(max_h, tensor.shape[1])\n",
    "            max_w = max(max_w, tensor.shape[2])\n",
    "\n",
    "        padded_imgs = [\n",
    "            pad(img, (0, max_w - img.shape[2], 0, max_h - img.shape[1]), mode='constant', value=0)\n",
    "            for img in images\n",
    "        ]\n",
    "        return torch.stack(padded_imgs), torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb934755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grouped_batches(txt_path, batch_size):\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "    entries = []\n",
    "    for path, label in lines:\n",
    "        with Image.open(path) as img:\n",
    "            w, h = img.size\n",
    "        entries.append((path, int(label), max(w, h)))\n",
    "\n",
    "    entries.sort(key=lambda x: x[2])\n",
    "    return [ [(p, l) for p, l, _ in entries[i:i+batch_size]]\n",
    "             for i in range(0, len(entries), batch_size)\n",
    "             if len(entries[i:i+batch_size]) == batch_size ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46cd0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦é©—çµ„å°ˆç”¨DataLoader\n",
    "bs = 8\n",
    "train_batches = make_grouped_batches(\"train.txt\", batch_size=bs)\n",
    "train_dataset = BatchListDataset(train_batches)\n",
    "train_exp_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_batches = make_grouped_batches(\"val.txt\", batch_size=bs)\n",
    "val_dataset = BatchListDataset(val_batches)\n",
    "val_exp_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_batches = make_grouped_batches(\"test.txt\", batch_size=bs)\n",
    "test_dataset = BatchListDataset(test_batches)\n",
    "test_exp_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd5b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯„ä¾‹æ¨¡å‹: AlexNet\n",
    "from torchvision import models\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, feature_module=False):\n",
    "    model.eval()\n",
    "    correct, total, total_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            if feature_module:\n",
    "                images = images.squeeze(0)\n",
    "                labels = labels.squeeze(0)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc = correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    return acc, avg_loss\n",
    "\n",
    "def train_model(train_loader, val_loader, feature_module=True, num_classes=50, epochs=10, lr=0.000005):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # ä½¿ç”¨ AlexNet ä¸¦æ›¿æ›ç¬¬ä¸€å±¤èˆ‡æœ€å¾Œä¸€å±¤\n",
    "    alexnet = models.alexnet(pretrained=False)\n",
    "    alexnet.features[0] = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "    alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "    if feature_module:\n",
    "        module = ChannelWiseInceptionAggregator()\n",
    "        model = nn.Sequential(module, alexnet)\n",
    "        print('Experiment group: Dynamic Convolution')\n",
    "    else:\n",
    "        model = alexnet\n",
    "        print('Control group: Static Convolution')\n",
    "        \n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            if feature_module:\n",
    "                images = images.squeeze(0)\n",
    "                labels = labels.squeeze(0)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # ğŸ”„ æ¯ 10 å€‹ batch å°ä¸€æ¬¡é€²åº¦ï¼ˆå¯èª¿æ•´ï¼‰\n",
    "            if (batch_idx + 1) % 50 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                print(f\"  Epoch {epoch+1}/{epochs} â”ƒ Batch {batch_idx+1}/{len(train_loader)} â”ƒ Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        val_acc, val_loss = evaluate(model, val_loader, criterion, device, feature_module)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Acc = {val_acc:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a465b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Control Group] Training with resized input\n",
      "Control group: Static Convolution\n",
      "  Epoch 1/10 â”ƒ Batch 50/1979 â”ƒ Loss: 3.9125\n",
      "  Epoch 1/10 â”ƒ Batch 100/1979 â”ƒ Loss: 3.9119\n",
      "  Epoch 1/10 â”ƒ Batch 150/1979 â”ƒ Loss: 3.9162\n",
      "  Epoch 1/10 â”ƒ Batch 200/1979 â”ƒ Loss: 3.9142\n",
      "  Epoch 1/10 â”ƒ Batch 250/1979 â”ƒ Loss: 3.9144\n",
      "  Epoch 1/10 â”ƒ Batch 300/1979 â”ƒ Loss: 3.9128\n",
      "  Epoch 1/10 â”ƒ Batch 350/1979 â”ƒ Loss: 3.9117\n",
      "  Epoch 1/10 â”ƒ Batch 400/1979 â”ƒ Loss: 3.9154\n",
      "  Epoch 1/10 â”ƒ Batch 450/1979 â”ƒ Loss: 3.9131\n",
      "  Epoch 1/10 â”ƒ Batch 500/1979 â”ƒ Loss: 3.9131\n",
      "  Epoch 1/10 â”ƒ Batch 550/1979 â”ƒ Loss: 3.9034\n",
      "  Epoch 1/10 â”ƒ Batch 600/1979 â”ƒ Loss: 3.9076\n",
      "  Epoch 1/10 â”ƒ Batch 650/1979 â”ƒ Loss: 3.9005\n",
      "  Epoch 1/10 â”ƒ Batch 700/1979 â”ƒ Loss: 3.9281\n",
      "  Epoch 1/10 â”ƒ Batch 750/1979 â”ƒ Loss: 3.7819\n",
      "  Epoch 1/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.7388\n",
      "  Epoch 1/10 â”ƒ Batch 850/1979 â”ƒ Loss: 3.8383\n",
      "  Epoch 1/10 â”ƒ Batch 900/1979 â”ƒ Loss: 3.7430\n",
      "  Epoch 1/10 â”ƒ Batch 950/1979 â”ƒ Loss: 3.6555\n",
      "  Epoch 1/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 3.6819\n",
      "  Epoch 1/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 3.7780\n",
      "  Epoch 1/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 3.9075\n",
      "  Epoch 1/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 3.6717\n",
      "  Epoch 1/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 3.8563\n",
      "  Epoch 1/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 3.6574\n",
      "  Epoch 1/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 3.6613\n",
      "  Epoch 1/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 3.6047\n",
      "  Epoch 1/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 3.7153\n",
      "  Epoch 1/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 3.6325\n",
      "  Epoch 1/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 3.6860\n",
      "  Epoch 1/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 3.7944\n",
      "  Epoch 1/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 3.4827\n",
      "  Epoch 1/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 3.7595\n",
      "  Epoch 1/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 3.7162\n",
      "  Epoch 1/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 3.5450\n",
      "  Epoch 1/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 3.6465\n",
      "  Epoch 1/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 3.5070\n",
      "  Epoch 1/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 3.5167\n",
      "  Epoch 1/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 3.6816\n",
      "  Epoch 1/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 4.0011\n",
      "Epoch 1: Train Loss = 3.7745, Val Acc = 0.0578, Val Loss = 3.6400\n",
      "  Epoch 2/10 â”ƒ Batch 50/1979 â”ƒ Loss: 3.5870\n",
      "  Epoch 2/10 â”ƒ Batch 100/1979 â”ƒ Loss: 3.5603\n",
      "  Epoch 2/10 â”ƒ Batch 150/1979 â”ƒ Loss: 3.5138\n",
      "  Epoch 2/10 â”ƒ Batch 200/1979 â”ƒ Loss: 3.6636\n",
      "  Epoch 2/10 â”ƒ Batch 250/1979 â”ƒ Loss: 3.5534\n",
      "  Epoch 2/10 â”ƒ Batch 300/1979 â”ƒ Loss: 3.7776\n",
      "  Epoch 2/10 â”ƒ Batch 350/1979 â”ƒ Loss: 3.6415\n",
      "  Epoch 2/10 â”ƒ Batch 400/1979 â”ƒ Loss: 3.5492\n",
      "  Epoch 2/10 â”ƒ Batch 450/1979 â”ƒ Loss: 3.4624\n",
      "  Epoch 2/10 â”ƒ Batch 500/1979 â”ƒ Loss: 3.4420\n",
      "  Epoch 2/10 â”ƒ Batch 550/1979 â”ƒ Loss: 3.6080\n",
      "  Epoch 2/10 â”ƒ Batch 600/1979 â”ƒ Loss: 3.5288\n",
      "  Epoch 2/10 â”ƒ Batch 650/1979 â”ƒ Loss: 3.3954\n",
      "  Epoch 2/10 â”ƒ Batch 700/1979 â”ƒ Loss: 3.6913\n",
      "  Epoch 2/10 â”ƒ Batch 750/1979 â”ƒ Loss: 3.6103\n",
      "  Epoch 2/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.4617\n",
      "  Epoch 2/10 â”ƒ Batch 850/1979 â”ƒ Loss: 3.7550\n",
      "  Epoch 2/10 â”ƒ Batch 900/1979 â”ƒ Loss: 3.5507\n",
      "  Epoch 2/10 â”ƒ Batch 950/1979 â”ƒ Loss: 3.5481\n",
      "  Epoch 2/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 3.4805\n",
      "  Epoch 2/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 3.5456\n",
      "  Epoch 2/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 3.3479\n",
      "  Epoch 2/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 3.5192\n",
      "  Epoch 2/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 3.5020\n",
      "  Epoch 2/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 3.4716\n",
      "  Epoch 2/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 3.6082\n",
      "  Epoch 2/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 3.4837\n",
      "  Epoch 2/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 3.5397\n",
      "  Epoch 2/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 3.3854\n",
      "  Epoch 2/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 3.6818\n",
      "  Epoch 2/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 3.4973\n",
      "  Epoch 2/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 3.4868\n",
      "  Epoch 2/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 3.4801\n",
      "  Epoch 2/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 3.6356\n",
      "  Epoch 2/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 3.4339\n",
      "  Epoch 2/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 3.4074\n",
      "  Epoch 2/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 3.3682\n",
      "  Epoch 2/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 3.4044\n",
      "  Epoch 2/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 3.1306\n",
      "  Epoch 2/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 3.1680\n",
      "Epoch 2: Train Loss = 3.4956, Val Acc = 0.1222, Val Loss = 3.4042\n",
      "  Epoch 3/10 â”ƒ Batch 50/1979 â”ƒ Loss: 3.1938\n",
      "  Epoch 3/10 â”ƒ Batch 100/1979 â”ƒ Loss: 3.3707\n",
      "  Epoch 3/10 â”ƒ Batch 150/1979 â”ƒ Loss: 3.3138\n",
      "  Epoch 3/10 â”ƒ Batch 200/1979 â”ƒ Loss: 3.2534\n",
      "  Epoch 3/10 â”ƒ Batch 250/1979 â”ƒ Loss: 3.0805\n",
      "  Epoch 3/10 â”ƒ Batch 300/1979 â”ƒ Loss: 3.2824\n",
      "  Epoch 3/10 â”ƒ Batch 350/1979 â”ƒ Loss: 3.3182\n",
      "  Epoch 3/10 â”ƒ Batch 400/1979 â”ƒ Loss: 3.1131\n",
      "  Epoch 3/10 â”ƒ Batch 450/1979 â”ƒ Loss: 3.5212\n",
      "  Epoch 3/10 â”ƒ Batch 500/1979 â”ƒ Loss: 3.4776\n",
      "  Epoch 3/10 â”ƒ Batch 550/1979 â”ƒ Loss: 3.4070\n",
      "  Epoch 3/10 â”ƒ Batch 600/1979 â”ƒ Loss: 3.2373\n",
      "  Epoch 3/10 â”ƒ Batch 650/1979 â”ƒ Loss: 3.3275\n",
      "  Epoch 3/10 â”ƒ Batch 700/1979 â”ƒ Loss: 3.4062\n",
      "  Epoch 3/10 â”ƒ Batch 750/1979 â”ƒ Loss: 3.0739\n",
      "  Epoch 3/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.3797\n",
      "  Epoch 3/10 â”ƒ Batch 850/1979 â”ƒ Loss: 3.6392\n",
      "  Epoch 3/10 â”ƒ Batch 900/1979 â”ƒ Loss: 3.4186\n",
      "  Epoch 3/10 â”ƒ Batch 950/1979 â”ƒ Loss: 3.3619\n",
      "  Epoch 3/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 3.2829\n",
      "  Epoch 3/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 3.4032\n",
      "  Epoch 3/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 3.3348\n",
      "  Epoch 3/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 3.1229\n",
      "  Epoch 3/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 3.4046\n",
      "  Epoch 3/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 3.5498\n",
      "  Epoch 3/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 3.3807\n",
      "  Epoch 3/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 3.4579\n",
      "  Epoch 3/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 3.0794\n",
      "  Epoch 3/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 3.2551\n",
      "  Epoch 3/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 3.4040\n",
      "  Epoch 3/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 3.2158\n",
      "  Epoch 3/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 3.4018\n",
      "  Epoch 3/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 3.2201\n",
      "  Epoch 3/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 3.3071\n",
      "  Epoch 3/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 3.2194\n",
      "  Epoch 3/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 3.1997\n",
      "  Epoch 3/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 3.4247\n",
      "  Epoch 3/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 3.1397\n",
      "  Epoch 3/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 3.0164\n",
      "  Epoch 3/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 3.1319\n",
      "Epoch 3: Train Loss = 3.3037, Val Acc = 0.1200, Val Loss = 3.2965\n",
      "  Epoch 4/10 â”ƒ Batch 50/1979 â”ƒ Loss: 3.3593\n",
      "  Epoch 4/10 â”ƒ Batch 100/1979 â”ƒ Loss: 3.4828\n",
      "  Epoch 4/10 â”ƒ Batch 150/1979 â”ƒ Loss: 3.2511\n",
      "  Epoch 4/10 â”ƒ Batch 200/1979 â”ƒ Loss: 3.3342\n",
      "  Epoch 4/10 â”ƒ Batch 250/1979 â”ƒ Loss: 2.8785\n",
      "  Epoch 4/10 â”ƒ Batch 300/1979 â”ƒ Loss: 3.2174\n",
      "  Epoch 4/10 â”ƒ Batch 350/1979 â”ƒ Loss: 3.4174\n",
      "  Epoch 4/10 â”ƒ Batch 400/1979 â”ƒ Loss: 3.0140\n",
      "  Epoch 4/10 â”ƒ Batch 450/1979 â”ƒ Loss: 3.4237\n",
      "  Epoch 4/10 â”ƒ Batch 500/1979 â”ƒ Loss: 3.5121\n",
      "  Epoch 4/10 â”ƒ Batch 550/1979 â”ƒ Loss: 2.8701\n",
      "  Epoch 4/10 â”ƒ Batch 600/1979 â”ƒ Loss: 3.1522\n",
      "  Epoch 4/10 â”ƒ Batch 650/1979 â”ƒ Loss: 2.8686\n",
      "  Epoch 4/10 â”ƒ Batch 700/1979 â”ƒ Loss: 2.9352\n",
      "  Epoch 4/10 â”ƒ Batch 750/1979 â”ƒ Loss: 3.1909\n",
      "  Epoch 4/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.4979\n",
      "  Epoch 4/10 â”ƒ Batch 850/1979 â”ƒ Loss: 3.2535\n",
      "  Epoch 4/10 â”ƒ Batch 900/1979 â”ƒ Loss: 3.5748\n",
      "  Epoch 4/10 â”ƒ Batch 950/1979 â”ƒ Loss: 3.1123\n",
      "  Epoch 4/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 3.2398\n",
      "  Epoch 4/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 3.2269\n",
      "  Epoch 4/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 3.1535\n",
      "  Epoch 4/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 3.1903\n",
      "  Epoch 4/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 3.3519\n",
      "  Epoch 4/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 3.3726\n",
      "  Epoch 4/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 3.1074\n",
      "  Epoch 4/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 3.2012\n",
      "  Epoch 4/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 3.4181\n",
      "  Epoch 4/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 3.0673\n",
      "  Epoch 4/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 3.3142\n",
      "  Epoch 4/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 2.8425\n",
      "  Epoch 4/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 3.2322\n",
      "  Epoch 4/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 3.2435\n",
      "  Epoch 4/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 3.0795\n",
      "  Epoch 4/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 3.1996\n",
      "  Epoch 4/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 3.0433\n",
      "  Epoch 4/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 2.8338\n",
      "  Epoch 4/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 3.4546\n",
      "  Epoch 4/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 3.2701\n",
      "  Epoch 4/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 3.5564\n",
      "Epoch 4: Train Loss = 3.2091, Val Acc = 0.1378, Val Loss = 3.2085\n",
      "  Epoch 5/10 â”ƒ Batch 50/1979 â”ƒ Loss: 3.2691\n",
      "  Epoch 5/10 â”ƒ Batch 100/1979 â”ƒ Loss: 2.7940\n",
      "  Epoch 5/10 â”ƒ Batch 150/1979 â”ƒ Loss: 3.0115\n",
      "  Epoch 5/10 â”ƒ Batch 200/1979 â”ƒ Loss: 3.2464\n",
      "  Epoch 5/10 â”ƒ Batch 250/1979 â”ƒ Loss: 3.2012\n",
      "  Epoch 5/10 â”ƒ Batch 300/1979 â”ƒ Loss: 3.1444\n",
      "  Epoch 5/10 â”ƒ Batch 350/1979 â”ƒ Loss: 2.9250\n",
      "  Epoch 5/10 â”ƒ Batch 400/1979 â”ƒ Loss: 2.9790\n",
      "  Epoch 5/10 â”ƒ Batch 450/1979 â”ƒ Loss: 3.2919\n",
      "  Epoch 5/10 â”ƒ Batch 500/1979 â”ƒ Loss: 2.4657\n",
      "  Epoch 5/10 â”ƒ Batch 550/1979 â”ƒ Loss: 3.2718\n",
      "  Epoch 5/10 â”ƒ Batch 600/1979 â”ƒ Loss: 3.0614\n",
      "  Epoch 5/10 â”ƒ Batch 650/1979 â”ƒ Loss: 3.0374\n",
      "  Epoch 5/10 â”ƒ Batch 700/1979 â”ƒ Loss: 2.8579\n",
      "  Epoch 5/10 â”ƒ Batch 750/1979 â”ƒ Loss: 3.0881\n",
      "  Epoch 5/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.3073\n",
      "  Epoch 5/10 â”ƒ Batch 850/1979 â”ƒ Loss: 3.1925\n",
      "  Epoch 5/10 â”ƒ Batch 900/1979 â”ƒ Loss: 2.8937\n",
      "  Epoch 5/10 â”ƒ Batch 950/1979 â”ƒ Loss: 3.2800\n",
      "  Epoch 5/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 3.1919\n",
      "  Epoch 5/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 3.6249\n",
      "  Epoch 5/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 3.2685\n",
      "  Epoch 5/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 3.2749\n",
      "  Epoch 5/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 2.8979\n",
      "  Epoch 5/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 3.0585\n",
      "  Epoch 5/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 3.2145\n",
      "  Epoch 5/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 3.0851\n",
      "  Epoch 5/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 3.0172\n",
      "  Epoch 5/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 3.1329\n",
      "  Epoch 5/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 3.4032\n",
      "  Epoch 5/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 3.2738\n",
      "  Epoch 5/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 2.7422\n",
      "  Epoch 5/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 2.9874\n",
      "  Epoch 5/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 2.6568\n",
      "  Epoch 5/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 3.2353\n",
      "  Epoch 5/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 3.1027\n",
      "  Epoch 5/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 3.3268\n",
      "  Epoch 5/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 3.0454\n",
      "  Epoch 5/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 3.2366\n",
      "  Epoch 5/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 3.2094\n",
      "Epoch 5: Train Loss = 3.1402, Val Acc = 0.1756, Val Loss = 3.1252\n",
      "  Epoch 6/10 â”ƒ Batch 50/1979 â”ƒ Loss: 3.1687\n",
      "  Epoch 6/10 â”ƒ Batch 100/1979 â”ƒ Loss: 3.0314\n",
      "  Epoch 6/10 â”ƒ Batch 150/1979 â”ƒ Loss: 3.0571\n",
      "  Epoch 6/10 â”ƒ Batch 200/1979 â”ƒ Loss: 2.9967\n",
      "  Epoch 6/10 â”ƒ Batch 250/1979 â”ƒ Loss: 2.9756\n",
      "  Epoch 6/10 â”ƒ Batch 300/1979 â”ƒ Loss: 2.9671\n",
      "  Epoch 6/10 â”ƒ Batch 350/1979 â”ƒ Loss: 2.8525\n",
      "  Epoch 6/10 â”ƒ Batch 400/1979 â”ƒ Loss: 2.9906\n",
      "  Epoch 6/10 â”ƒ Batch 450/1979 â”ƒ Loss: 3.1626\n",
      "  Epoch 6/10 â”ƒ Batch 500/1979 â”ƒ Loss: 3.0087\n",
      "  Epoch 6/10 â”ƒ Batch 550/1979 â”ƒ Loss: 3.3078\n",
      "  Epoch 6/10 â”ƒ Batch 600/1979 â”ƒ Loss: 2.9546\n",
      "  Epoch 6/10 â”ƒ Batch 650/1979 â”ƒ Loss: 3.3690\n",
      "  Epoch 6/10 â”ƒ Batch 700/1979 â”ƒ Loss: 3.4483\n",
      "  Epoch 6/10 â”ƒ Batch 750/1979 â”ƒ Loss: 2.9197\n",
      "  Epoch 6/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.2019\n",
      "  Epoch 6/10 â”ƒ Batch 850/1979 â”ƒ Loss: 3.3889\n",
      "  Epoch 6/10 â”ƒ Batch 900/1979 â”ƒ Loss: 3.0213\n",
      "  Epoch 6/10 â”ƒ Batch 950/1979 â”ƒ Loss: 2.9732\n",
      "  Epoch 6/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 2.7849\n",
      "  Epoch 6/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 2.8773\n",
      "  Epoch 6/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 3.0998\n",
      "  Epoch 6/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 3.2370\n",
      "  Epoch 6/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 3.0704\n",
      "  Epoch 6/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 3.2059\n",
      "  Epoch 6/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 2.6556\n",
      "  Epoch 6/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 3.1076\n",
      "  Epoch 6/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 3.0756\n",
      "  Epoch 6/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 2.8549\n",
      "  Epoch 6/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 2.7556\n",
      "  Epoch 6/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 2.9303\n",
      "  Epoch 6/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 3.3831\n",
      "  Epoch 6/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 2.7917\n",
      "  Epoch 6/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 3.0323\n",
      "  Epoch 6/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 3.3981\n",
      "  Epoch 6/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 2.8831\n",
      "  Epoch 6/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 3.1417\n",
      "  Epoch 6/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 3.4950\n",
      "  Epoch 6/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 3.1086\n",
      "  Epoch 6/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 3.2358\n",
      "Epoch 6: Train Loss = 3.0895, Val Acc = 0.1778, Val Loss = 3.0707\n",
      "  Epoch 7/10 â”ƒ Batch 50/1979 â”ƒ Loss: 3.2306\n",
      "  Epoch 7/10 â”ƒ Batch 100/1979 â”ƒ Loss: 2.7911\n",
      "  Epoch 7/10 â”ƒ Batch 150/1979 â”ƒ Loss: 3.2496\n",
      "  Epoch 7/10 â”ƒ Batch 200/1979 â”ƒ Loss: 3.0296\n",
      "  Epoch 7/10 â”ƒ Batch 250/1979 â”ƒ Loss: 2.9040\n",
      "  Epoch 7/10 â”ƒ Batch 300/1979 â”ƒ Loss: 2.9452\n",
      "  Epoch 7/10 â”ƒ Batch 350/1979 â”ƒ Loss: 3.0601\n",
      "  Epoch 7/10 â”ƒ Batch 400/1979 â”ƒ Loss: 3.2988\n",
      "  Epoch 7/10 â”ƒ Batch 450/1979 â”ƒ Loss: 2.8488\n",
      "  Epoch 7/10 â”ƒ Batch 500/1979 â”ƒ Loss: 2.8621\n",
      "  Epoch 7/10 â”ƒ Batch 550/1979 â”ƒ Loss: 2.8460\n",
      "  Epoch 7/10 â”ƒ Batch 600/1979 â”ƒ Loss: 2.9488\n",
      "  Epoch 7/10 â”ƒ Batch 650/1979 â”ƒ Loss: 2.9701\n",
      "  Epoch 7/10 â”ƒ Batch 700/1979 â”ƒ Loss: 2.8676\n",
      "  Epoch 7/10 â”ƒ Batch 750/1979 â”ƒ Loss: 2.9118\n",
      "  Epoch 7/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.0201\n",
      "  Epoch 7/10 â”ƒ Batch 850/1979 â”ƒ Loss: 2.8104\n",
      "  Epoch 7/10 â”ƒ Batch 900/1979 â”ƒ Loss: 3.0147\n",
      "  Epoch 7/10 â”ƒ Batch 950/1979 â”ƒ Loss: 2.7767\n",
      "  Epoch 7/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 2.7392\n",
      "  Epoch 7/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 2.9253\n",
      "  Epoch 7/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 3.0402\n",
      "  Epoch 7/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 2.8938\n",
      "  Epoch 7/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 3.1543\n",
      "  Epoch 7/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 3.1444\n",
      "  Epoch 7/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 2.9146\n",
      "  Epoch 7/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 3.3214\n",
      "  Epoch 7/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 3.1282\n",
      "  Epoch 7/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 3.4743\n",
      "  Epoch 7/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 3.3074\n",
      "  Epoch 7/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 3.3306\n",
      "  Epoch 7/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 3.1308\n",
      "  Epoch 7/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 2.6353\n",
      "  Epoch 7/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 3.1963\n",
      "  Epoch 7/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 3.2783\n",
      "  Epoch 7/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 2.8222\n",
      "  Epoch 7/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 2.9702\n",
      "  Epoch 7/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 2.8384\n",
      "  Epoch 7/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 3.2022\n",
      "  Epoch 7/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 3.1314\n",
      "Epoch 7: Train Loss = 3.0428, Val Acc = 0.1844, Val Loss = 3.0393\n",
      "  Epoch 8/10 â”ƒ Batch 50/1979 â”ƒ Loss: 2.9157\n",
      "  Epoch 8/10 â”ƒ Batch 100/1979 â”ƒ Loss: 2.6785\n",
      "  Epoch 8/10 â”ƒ Batch 150/1979 â”ƒ Loss: 3.2682\n",
      "  Epoch 8/10 â”ƒ Batch 200/1979 â”ƒ Loss: 2.8075\n",
      "  Epoch 8/10 â”ƒ Batch 250/1979 â”ƒ Loss: 3.0654\n",
      "  Epoch 8/10 â”ƒ Batch 300/1979 â”ƒ Loss: 2.9230\n",
      "  Epoch 8/10 â”ƒ Batch 350/1979 â”ƒ Loss: 2.9008\n",
      "  Epoch 8/10 â”ƒ Batch 400/1979 â”ƒ Loss: 2.6372\n",
      "  Epoch 8/10 â”ƒ Batch 450/1979 â”ƒ Loss: 3.2264\n",
      "  Epoch 8/10 â”ƒ Batch 500/1979 â”ƒ Loss: 3.1532\n",
      "  Epoch 8/10 â”ƒ Batch 550/1979 â”ƒ Loss: 3.0450\n",
      "  Epoch 8/10 â”ƒ Batch 600/1979 â”ƒ Loss: 3.1406\n",
      "  Epoch 8/10 â”ƒ Batch 650/1979 â”ƒ Loss: 3.1098\n",
      "  Epoch 8/10 â”ƒ Batch 700/1979 â”ƒ Loss: 3.0976\n",
      "  Epoch 8/10 â”ƒ Batch 750/1979 â”ƒ Loss: 3.0136\n",
      "  Epoch 8/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.0613\n",
      "  Epoch 8/10 â”ƒ Batch 850/1979 â”ƒ Loss: 3.0910\n",
      "  Epoch 8/10 â”ƒ Batch 900/1979 â”ƒ Loss: 3.1678\n",
      "  Epoch 8/10 â”ƒ Batch 950/1979 â”ƒ Loss: 2.8480\n",
      "  Epoch 8/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 3.1220\n",
      "  Epoch 8/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 2.9412\n",
      "  Epoch 8/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 2.8438\n",
      "  Epoch 8/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 3.0133\n",
      "  Epoch 8/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 2.7973\n",
      "  Epoch 8/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 3.1367\n",
      "  Epoch 8/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 3.1018\n",
      "  Epoch 8/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 2.8343\n",
      "  Epoch 8/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 2.9055\n",
      "  Epoch 8/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 3.1269\n",
      "  Epoch 8/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 3.0030\n",
      "  Epoch 8/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 3.0650\n",
      "  Epoch 8/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 3.1159\n",
      "  Epoch 8/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 2.8482\n",
      "  Epoch 8/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 3.1800\n",
      "  Epoch 8/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 2.9873\n",
      "  Epoch 8/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 2.6696\n",
      "  Epoch 8/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 3.0281\n",
      "  Epoch 8/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 3.1094\n",
      "  Epoch 8/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 2.9516\n",
      "  Epoch 8/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 2.8797\n",
      "Epoch 8: Train Loss = 3.0024, Val Acc = 0.2089, Val Loss = 3.0040\n",
      "  Epoch 9/10 â”ƒ Batch 50/1979 â”ƒ Loss: 2.5841\n",
      "  Epoch 9/10 â”ƒ Batch 100/1979 â”ƒ Loss: 2.9389\n",
      "  Epoch 9/10 â”ƒ Batch 150/1979 â”ƒ Loss: 2.8766\n",
      "  Epoch 9/10 â”ƒ Batch 200/1979 â”ƒ Loss: 3.0602\n",
      "  Epoch 9/10 â”ƒ Batch 250/1979 â”ƒ Loss: 3.0356\n",
      "  Epoch 9/10 â”ƒ Batch 300/1979 â”ƒ Loss: 2.9314\n",
      "  Epoch 9/10 â”ƒ Batch 350/1979 â”ƒ Loss: 3.2726\n",
      "  Epoch 9/10 â”ƒ Batch 400/1979 â”ƒ Loss: 2.9387\n",
      "  Epoch 9/10 â”ƒ Batch 450/1979 â”ƒ Loss: 3.1592\n",
      "  Epoch 9/10 â”ƒ Batch 500/1979 â”ƒ Loss: 2.8686\n",
      "  Epoch 9/10 â”ƒ Batch 550/1979 â”ƒ Loss: 2.7076\n",
      "  Epoch 9/10 â”ƒ Batch 600/1979 â”ƒ Loss: 3.1983\n",
      "  Epoch 9/10 â”ƒ Batch 650/1979 â”ƒ Loss: 3.3075\n",
      "  Epoch 9/10 â”ƒ Batch 700/1979 â”ƒ Loss: 2.9391\n",
      "  Epoch 9/10 â”ƒ Batch 750/1979 â”ƒ Loss: 3.2744\n",
      "  Epoch 9/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.5937\n",
      "  Epoch 9/10 â”ƒ Batch 850/1979 â”ƒ Loss: 2.6817\n",
      "  Epoch 9/10 â”ƒ Batch 900/1979 â”ƒ Loss: 2.9192\n",
      "  Epoch 9/10 â”ƒ Batch 950/1979 â”ƒ Loss: 2.8560\n",
      "  Epoch 9/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 3.4625\n",
      "  Epoch 9/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 2.6942\n",
      "  Epoch 9/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 3.1874\n",
      "  Epoch 9/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 2.9678\n",
      "  Epoch 9/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 2.9761\n",
      "  Epoch 9/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 3.2473\n",
      "  Epoch 9/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 2.9370\n",
      "  Epoch 9/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 3.1399\n",
      "  Epoch 9/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 2.8972\n",
      "  Epoch 9/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 2.9580\n",
      "  Epoch 9/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 2.8344\n",
      "  Epoch 9/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 3.2141\n",
      "  Epoch 9/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 3.0517\n",
      "  Epoch 9/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 2.7695\n",
      "  Epoch 9/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 2.8669\n",
      "  Epoch 9/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 2.6088\n",
      "  Epoch 9/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 2.7430\n",
      "  Epoch 9/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 2.4491\n",
      "  Epoch 9/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 3.1581\n",
      "  Epoch 9/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 3.2196\n",
      "  Epoch 9/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 2.7242\n",
      "Epoch 9: Train Loss = 2.9654, Val Acc = 0.1867, Val Loss = 2.9724\n",
      "  Epoch 10/10 â”ƒ Batch 50/1979 â”ƒ Loss: 3.3612\n",
      "  Epoch 10/10 â”ƒ Batch 100/1979 â”ƒ Loss: 2.8114\n",
      "  Epoch 10/10 â”ƒ Batch 150/1979 â”ƒ Loss: 2.7083\n",
      "  Epoch 10/10 â”ƒ Batch 200/1979 â”ƒ Loss: 3.1308\n",
      "  Epoch 10/10 â”ƒ Batch 250/1979 â”ƒ Loss: 2.7870\n",
      "  Epoch 10/10 â”ƒ Batch 300/1979 â”ƒ Loss: 3.1202\n",
      "  Epoch 10/10 â”ƒ Batch 350/1979 â”ƒ Loss: 2.8032\n",
      "  Epoch 10/10 â”ƒ Batch 400/1979 â”ƒ Loss: 2.7475\n",
      "  Epoch 10/10 â”ƒ Batch 450/1979 â”ƒ Loss: 2.6304\n",
      "  Epoch 10/10 â”ƒ Batch 500/1979 â”ƒ Loss: 2.8993\n",
      "  Epoch 10/10 â”ƒ Batch 550/1979 â”ƒ Loss: 2.6367\n",
      "  Epoch 10/10 â”ƒ Batch 600/1979 â”ƒ Loss: 2.9495\n",
      "  Epoch 10/10 â”ƒ Batch 650/1979 â”ƒ Loss: 2.6889\n",
      "  Epoch 10/10 â”ƒ Batch 700/1979 â”ƒ Loss: 2.9495\n",
      "  Epoch 10/10 â”ƒ Batch 750/1979 â”ƒ Loss: 2.9100\n",
      "  Epoch 10/10 â”ƒ Batch 800/1979 â”ƒ Loss: 3.0183\n",
      "  Epoch 10/10 â”ƒ Batch 850/1979 â”ƒ Loss: 3.3275\n",
      "  Epoch 10/10 â”ƒ Batch 900/1979 â”ƒ Loss: 3.1579\n",
      "  Epoch 10/10 â”ƒ Batch 950/1979 â”ƒ Loss: 2.9007\n",
      "  Epoch 10/10 â”ƒ Batch 1000/1979 â”ƒ Loss: 3.1353\n",
      "  Epoch 10/10 â”ƒ Batch 1050/1979 â”ƒ Loss: 2.8688\n",
      "  Epoch 10/10 â”ƒ Batch 1100/1979 â”ƒ Loss: 2.8969\n",
      "  Epoch 10/10 â”ƒ Batch 1150/1979 â”ƒ Loss: 3.0692\n",
      "  Epoch 10/10 â”ƒ Batch 1200/1979 â”ƒ Loss: 2.9359\n",
      "  Epoch 10/10 â”ƒ Batch 1250/1979 â”ƒ Loss: 2.7992\n",
      "  Epoch 10/10 â”ƒ Batch 1300/1979 â”ƒ Loss: 2.6311\n",
      "  Epoch 10/10 â”ƒ Batch 1350/1979 â”ƒ Loss: 2.7685\n",
      "  Epoch 10/10 â”ƒ Batch 1400/1979 â”ƒ Loss: 3.2918\n",
      "  Epoch 10/10 â”ƒ Batch 1450/1979 â”ƒ Loss: 2.8837\n",
      "  Epoch 10/10 â”ƒ Batch 1500/1979 â”ƒ Loss: 2.6725\n",
      "  Epoch 10/10 â”ƒ Batch 1550/1979 â”ƒ Loss: 3.0729\n",
      "  Epoch 10/10 â”ƒ Batch 1600/1979 â”ƒ Loss: 3.0695\n",
      "  Epoch 10/10 â”ƒ Batch 1650/1979 â”ƒ Loss: 2.6230\n",
      "  Epoch 10/10 â”ƒ Batch 1700/1979 â”ƒ Loss: 2.9784\n",
      "  Epoch 10/10 â”ƒ Batch 1750/1979 â”ƒ Loss: 2.3705\n",
      "  Epoch 10/10 â”ƒ Batch 1800/1979 â”ƒ Loss: 3.1109\n",
      "  Epoch 10/10 â”ƒ Batch 1850/1979 â”ƒ Loss: 3.2280\n",
      "  Epoch 10/10 â”ƒ Batch 1900/1979 â”ƒ Loss: 3.3088\n",
      "  Epoch 10/10 â”ƒ Batch 1950/1979 â”ƒ Loss: 2.7259\n",
      "  Epoch 10/10 â”ƒ Batch 1979/1979 â”ƒ Loss: 3.3064\n",
      "Epoch 10: Train Loss = 2.9301, Val Acc = 0.2067, Val Loss = 2.9386\n"
     ]
    }
   ],
   "source": [
    "# Controlçµ„è¨“ç·´ç¸½æ™‚é•·: 0:43:15\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"\\n[Control Group] Training with resized input\")\n",
    "model_ctrl = train_model(train_control_loader, val_control_loader, False)\n",
    "acc_ctrl, loss_ctrl = evaluate(model_ctrl, test_control_loader, nn.CrossEntropyLoss(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Experimental Group] Training with inception-based module\n",
      "Experiment group: Dynamic Convolution\n",
      "  Epoch 1/10 â”ƒ Batch 50/7915 â”ƒ Loss: 3.9154\n",
      "  Epoch 1/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.9096\n",
      "  Epoch 1/10 â”ƒ Batch 150/7915 â”ƒ Loss: 3.8984\n",
      "  Epoch 1/10 â”ƒ Batch 200/7915 â”ƒ Loss: 3.8969\n",
      "  Epoch 1/10 â”ƒ Batch 250/7915 â”ƒ Loss: 3.9094\n",
      "  Epoch 1/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.9175\n",
      "  Epoch 1/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.9063\n",
      "  Epoch 1/10 â”ƒ Batch 400/7915 â”ƒ Loss: 3.8995\n",
      "  Epoch 1/10 â”ƒ Batch 450/7915 â”ƒ Loss: 3.9083\n",
      "  Epoch 1/10 â”ƒ Batch 500/7915 â”ƒ Loss: 3.9043\n",
      "  Epoch 1/10 â”ƒ Batch 550/7915 â”ƒ Loss: 3.9022\n",
      "  Epoch 1/10 â”ƒ Batch 600/7915 â”ƒ Loss: 3.9112\n",
      "  Epoch 1/10 â”ƒ Batch 650/7915 â”ƒ Loss: 3.9155\n",
      "  Epoch 1/10 â”ƒ Batch 700/7915 â”ƒ Loss: 3.9030\n",
      "  Epoch 1/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.9174\n",
      "  Epoch 1/10 â”ƒ Batch 800/7915 â”ƒ Loss: 3.8970\n",
      "  Epoch 1/10 â”ƒ Batch 850/7915 â”ƒ Loss: 3.9173\n",
      "  Epoch 1/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.9207\n",
      "  Epoch 1/10 â”ƒ Batch 950/7915 â”ƒ Loss: 3.9039\n",
      "  Epoch 1/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 3.9092\n",
      "  Epoch 1/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 3.9165\n",
      "  Epoch 1/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 3.9237\n",
      "  Epoch 1/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 3.9091\n",
      "  Epoch 1/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.9112\n",
      "  Epoch 1/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.9096\n",
      "  Epoch 1/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 3.9139\n",
      "  Epoch 1/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 3.9247\n",
      "  Epoch 1/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 3.9172\n",
      "  Epoch 1/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.8964\n",
      "  Epoch 1/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.9230\n",
      "  Epoch 1/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 3.9223\n",
      "  Epoch 1/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 3.8977\n",
      "  Epoch 1/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.9017\n",
      "  Epoch 1/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 3.9146\n",
      "  Epoch 1/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 3.9119\n",
      "  Epoch 1/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.9108\n",
      "  Epoch 1/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 3.9114\n",
      "  Epoch 1/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 3.8897\n",
      "  Epoch 1/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 3.9212\n",
      "  Epoch 1/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 3.9117\n",
      "  Epoch 1/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 3.9140\n",
      "  Epoch 1/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 3.9247\n",
      "  Epoch 1/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 3.9146\n",
      "  Epoch 1/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 3.9180\n",
      "  Epoch 1/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 3.9040\n",
      "  Epoch 1/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.9117\n",
      "  Epoch 1/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.9138\n",
      "  Epoch 1/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.9205\n",
      "  Epoch 1/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.9237\n",
      "  Epoch 1/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.9186\n",
      "  Epoch 1/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 3.9073\n",
      "  Epoch 1/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 3.9203\n",
      "  Epoch 1/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 3.9146\n",
      "  Epoch 1/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 3.9050\n",
      "  Epoch 1/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 3.9024\n",
      "  Epoch 1/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 3.9097\n",
      "  Epoch 1/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 3.9153\n",
      "  Epoch 1/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 3.9256\n",
      "  Epoch 1/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 3.9147\n",
      "  Epoch 1/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 3.9250\n",
      "  Epoch 1/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.9207\n",
      "  Epoch 1/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 3.9187\n",
      "  Epoch 1/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 3.9347\n",
      "  Epoch 1/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.9249\n",
      "  Epoch 1/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.9122\n",
      "  Epoch 1/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 3.9126\n",
      "  Epoch 1/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 3.9526\n",
      "  Epoch 1/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 3.9096\n",
      "  Epoch 1/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 3.9141\n",
      "  Epoch 1/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 3.9118\n",
      "  Epoch 1/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 3.9070\n",
      "  Epoch 1/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 3.9238\n",
      "  Epoch 1/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 3.9239\n",
      "  Epoch 1/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 3.9197\n",
      "  Epoch 1/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 3.9193\n",
      "  Epoch 1/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 3.9031\n",
      "  Epoch 1/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 3.9092\n",
      "  Epoch 1/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.9157\n",
      "  Epoch 1/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.9152\n",
      "  Epoch 1/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.9119\n",
      "  Epoch 1/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 3.9256\n",
      "  Epoch 1/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 3.9204\n",
      "  Epoch 1/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 3.9097\n",
      "  Epoch 1/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 3.9195\n",
      "  Epoch 1/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.8994\n",
      "  Epoch 1/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.9062\n",
      "  Epoch 1/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 3.9189\n",
      "  Epoch 1/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 3.9028\n",
      "  Epoch 1/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 3.9212\n",
      "  Epoch 1/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 3.9082\n",
      "  Epoch 1/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.9142\n",
      "  Epoch 1/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.9059\n",
      "  Epoch 1/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.9059\n",
      "  Epoch 1/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 3.9143\n",
      "  Epoch 1/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 3.8949\n",
      "  Epoch 1/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 3.9067\n",
      "  Epoch 1/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 3.9013\n",
      "  Epoch 1/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 3.9026\n",
      "  Epoch 1/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 3.9069\n",
      "  Epoch 1/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 3.9071\n",
      "  Epoch 1/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 3.9713\n",
      "  Epoch 1/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.9666\n",
      "  Epoch 1/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 3.9084\n",
      "  Epoch 1/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 3.9552\n",
      "  Epoch 1/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.9670\n",
      "  Epoch 1/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 3.9118\n",
      "  Epoch 1/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 3.9047\n",
      "  Epoch 1/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.9187\n",
      "  Epoch 1/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.9069\n",
      "  Epoch 1/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 3.9126\n",
      "  Epoch 1/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.9234\n",
      "  Epoch 1/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 3.9085\n",
      "  Epoch 1/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 3.9025\n",
      "  Epoch 1/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 3.9034\n",
      "  Epoch 1/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 3.9220\n",
      "  Epoch 1/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 3.9145\n",
      "  Epoch 1/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 3.9009\n",
      "  Epoch 1/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.9085\n",
      "  Epoch 1/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 3.9060\n",
      "  Epoch 1/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 3.9115\n",
      "  Epoch 1/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 3.9095\n",
      "  Epoch 1/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.9351\n",
      "  Epoch 1/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 3.9052\n",
      "  Epoch 1/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.8989\n",
      "  Epoch 1/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 3.9116\n",
      "  Epoch 1/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 3.9132\n",
      "  Epoch 1/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.9088\n",
      "  Epoch 1/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 3.9199\n",
      "  Epoch 1/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 3.8914\n",
      "  Epoch 1/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 3.9164\n",
      "  Epoch 1/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 3.9154\n",
      "  Epoch 1/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 3.9198\n",
      "  Epoch 1/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.9133\n",
      "  Epoch 1/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 3.9023\n",
      "  Epoch 1/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 3.8949\n",
      "  Epoch 1/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 3.9271\n",
      "  Epoch 1/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 3.8976\n",
      "  Epoch 1/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 3.9030\n",
      "  Epoch 1/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.8996\n",
      "  Epoch 1/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 3.9073\n",
      "  Epoch 1/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 3.9015\n",
      "  Epoch 1/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 3.9058\n",
      "  Epoch 1/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.9130\n",
      "  Epoch 1/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 3.9126\n",
      "  Epoch 1/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 3.9102\n",
      "  Epoch 1/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 3.9191\n",
      "  Epoch 1/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 3.9107\n",
      "  Epoch 1/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 3.8957\n",
      "  Epoch 1/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 3.9135\n",
      "  Epoch 1/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 3.9216\n",
      "  Epoch 1/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 3.9044\n",
      "  Epoch 1/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 3.9035\n",
      "  Epoch 1/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 3.9067\n",
      "  Epoch 1/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 3.9107\n",
      "  Epoch 1/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 3.9041\n",
      "  Epoch 1/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.9117\n",
      "  Epoch 1/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 3.9023\n",
      "  Epoch 1/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 3.9015\n",
      "  Epoch 1/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.9076\n",
      "Epoch 1: Train Loss = 31.2953, Val Acc = 0.0201, Val Loss = 3.9116\n",
      "  Epoch 2/10 â”ƒ Batch 50/7915 â”ƒ Loss: 3.9016\n",
      "  Epoch 2/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.9122\n",
      "  Epoch 2/10 â”ƒ Batch 150/7915 â”ƒ Loss: 3.9070\n",
      "  Epoch 2/10 â”ƒ Batch 200/7915 â”ƒ Loss: 3.8997\n",
      "  Epoch 2/10 â”ƒ Batch 250/7915 â”ƒ Loss: 3.9075\n",
      "  Epoch 2/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.9033\n",
      "  Epoch 2/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.9063\n",
      "  Epoch 2/10 â”ƒ Batch 400/7915 â”ƒ Loss: 3.9062\n",
      "  Epoch 2/10 â”ƒ Batch 450/7915 â”ƒ Loss: 3.9076\n",
      "  Epoch 2/10 â”ƒ Batch 500/7915 â”ƒ Loss: 3.8973\n",
      "  Epoch 2/10 â”ƒ Batch 550/7915 â”ƒ Loss: 3.9075\n",
      "  Epoch 2/10 â”ƒ Batch 600/7915 â”ƒ Loss: 3.8988\n",
      "  Epoch 2/10 â”ƒ Batch 650/7915 â”ƒ Loss: 3.9039\n",
      "  Epoch 2/10 â”ƒ Batch 700/7915 â”ƒ Loss: 3.9113\n",
      "  Epoch 2/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.9362\n",
      "  Epoch 2/10 â”ƒ Batch 800/7915 â”ƒ Loss: 3.9243\n",
      "  Epoch 2/10 â”ƒ Batch 850/7915 â”ƒ Loss: 3.9309\n",
      "  Epoch 2/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.9230\n",
      "  Epoch 2/10 â”ƒ Batch 950/7915 â”ƒ Loss: 3.8964\n",
      "  Epoch 2/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 3.9160\n",
      "  Epoch 2/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 3.8965\n",
      "  Epoch 2/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 3.9042\n",
      "  Epoch 2/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 3.9117\n",
      "  Epoch 2/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.9095\n",
      "  Epoch 2/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.9148\n",
      "  Epoch 2/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 3.9035\n",
      "  Epoch 2/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 3.9078\n",
      "  Epoch 2/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 3.9030\n",
      "  Epoch 2/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.9210\n",
      "  Epoch 2/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.9118\n",
      "  Epoch 2/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 3.9590\n",
      "  Epoch 2/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 3.9138\n",
      "  Epoch 2/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.9094\n",
      "  Epoch 2/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 3.9045\n",
      "  Epoch 2/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 3.9104\n",
      "  Epoch 2/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.9046\n",
      "  Epoch 2/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 3.9340\n",
      "  Epoch 2/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 3.9241\n",
      "  Epoch 2/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 3.8827\n",
      "  Epoch 2/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 3.9114\n",
      "  Epoch 2/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 3.9056\n",
      "  Epoch 2/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 3.9086\n",
      "  Epoch 2/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 3.9000\n",
      "  Epoch 2/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 3.9157\n",
      "  Epoch 2/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 3.9188\n",
      "  Epoch 2/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.9455\n",
      "  Epoch 2/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.9038\n",
      "  Epoch 2/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.9283\n",
      "  Epoch 2/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.8969\n",
      "  Epoch 2/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.8975\n",
      "  Epoch 2/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 3.9052\n",
      "  Epoch 2/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 3.9154\n",
      "  Epoch 2/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 3.9159\n",
      "  Epoch 2/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 3.9234\n",
      "  Epoch 2/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 3.9087\n",
      "  Epoch 2/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 3.9241\n",
      "  Epoch 2/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 3.9097\n",
      "  Epoch 2/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 3.9353\n",
      "  Epoch 2/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 3.9150\n",
      "  Epoch 2/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 3.9085\n",
      "  Epoch 2/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.8895\n",
      "  Epoch 2/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 3.9171\n",
      "  Epoch 2/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 3.9085\n",
      "  Epoch 2/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.9224\n",
      "  Epoch 2/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.8668\n",
      "  Epoch 2/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 3.8912\n",
      "  Epoch 2/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 3.9231\n",
      "  Epoch 2/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 3.9427\n",
      "  Epoch 2/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 3.9188\n",
      "  Epoch 2/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 3.9726\n",
      "  Epoch 2/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 3.9414\n",
      "  Epoch 2/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 3.8737\n",
      "  Epoch 2/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 3.8252\n",
      "  Epoch 2/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 3.9032\n",
      "  Epoch 2/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 3.9140\n",
      "  Epoch 2/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 3.8384\n",
      "  Epoch 2/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 3.9049\n",
      "  Epoch 2/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.9035\n",
      "  Epoch 2/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.9096\n",
      "  Epoch 2/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.8619\n",
      "  Epoch 2/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 3.8788\n",
      "  Epoch 2/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 3.8969\n",
      "  Epoch 2/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 3.8459\n",
      "  Epoch 2/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 3.8958\n",
      "  Epoch 2/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.8228\n",
      "  Epoch 2/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.9883\n",
      "  Epoch 2/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 3.9014\n",
      "  Epoch 2/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 3.9298\n",
      "  Epoch 2/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 3.9167\n",
      "  Epoch 2/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 3.9603\n",
      "  Epoch 2/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.8915\n",
      "  Epoch 2/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.8616\n",
      "  Epoch 2/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.8687\n",
      "  Epoch 2/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 3.9595\n",
      "  Epoch 2/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 3.8353\n",
      "  Epoch 2/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 3.9419\n",
      "  Epoch 2/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 3.8859\n",
      "  Epoch 2/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 3.9484\n",
      "  Epoch 2/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 3.8857\n",
      "  Epoch 2/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 3.8379\n",
      "  Epoch 2/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 3.8810\n",
      "  Epoch 2/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.8641\n",
      "  Epoch 2/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 3.8770\n",
      "  Epoch 2/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 3.9810\n",
      "  Epoch 2/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.8884\n",
      "  Epoch 2/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 3.8969\n",
      "  Epoch 2/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 3.8559\n",
      "  Epoch 2/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.9228\n",
      "  Epoch 2/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.9017\n",
      "  Epoch 2/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 3.8608\n",
      "  Epoch 2/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.8060\n",
      "  Epoch 2/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 3.9464\n",
      "  Epoch 2/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 3.9487\n",
      "  Epoch 2/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 3.9506\n",
      "  Epoch 2/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 3.9388\n",
      "  Epoch 2/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 3.9092\n",
      "  Epoch 2/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 3.9174\n",
      "  Epoch 2/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.8108\n",
      "  Epoch 2/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 3.8264\n",
      "  Epoch 2/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 3.8935\n",
      "  Epoch 2/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 3.7741\n",
      "  Epoch 2/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.8772\n",
      "  Epoch 2/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 3.9270\n",
      "  Epoch 2/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.8590\n",
      "  Epoch 2/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 3.8528\n",
      "  Epoch 2/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 3.9228\n",
      "  Epoch 2/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.8961\n",
      "  Epoch 2/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 3.8884\n",
      "  Epoch 2/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 3.8720\n",
      "  Epoch 2/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 3.9358\n",
      "  Epoch 2/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 3.9202\n",
      "  Epoch 2/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 3.8730\n",
      "  Epoch 2/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.9135\n",
      "  Epoch 2/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 3.6098\n",
      "  Epoch 2/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 3.9236\n",
      "  Epoch 2/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 3.9484\n",
      "  Epoch 2/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 4.0331\n",
      "  Epoch 2/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 3.8141\n",
      "  Epoch 2/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.8589\n",
      "  Epoch 2/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 3.9985\n",
      "  Epoch 2/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 3.7495\n",
      "  Epoch 2/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 3.9084\n",
      "  Epoch 2/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.9892\n",
      "  Epoch 2/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 3.8839\n",
      "  Epoch 2/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 3.8178\n",
      "  Epoch 2/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 3.6822\n",
      "  Epoch 2/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 3.8631\n",
      "  Epoch 2/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 3.9790\n",
      "  Epoch 2/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 3.9485\n",
      "  Epoch 2/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 3.9172\n",
      "  Epoch 2/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 3.7744\n",
      "  Epoch 2/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 3.8621\n",
      "  Epoch 2/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 3.8312\n",
      "  Epoch 2/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 4.0660\n",
      "  Epoch 2/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 3.8658\n",
      "  Epoch 2/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.6809\n",
      "  Epoch 2/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 3.7563\n",
      "  Epoch 2/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 3.8036\n",
      "  Epoch 2/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.6447\n",
      "Epoch 2: Train Loss = 31.1936, Val Acc = 0.0513, Val Loss = 3.8698\n",
      "  Epoch 3/10 â”ƒ Batch 50/7915 â”ƒ Loss: 3.7690\n",
      "  Epoch 3/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.7481\n",
      "  Epoch 3/10 â”ƒ Batch 150/7915 â”ƒ Loss: 3.9318\n",
      "  Epoch 3/10 â”ƒ Batch 200/7915 â”ƒ Loss: 3.6699\n",
      "  Epoch 3/10 â”ƒ Batch 250/7915 â”ƒ Loss: 3.8414\n",
      "  Epoch 3/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.5815\n",
      "  Epoch 3/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.7410\n",
      "  Epoch 3/10 â”ƒ Batch 400/7915 â”ƒ Loss: 3.8256\n",
      "  Epoch 3/10 â”ƒ Batch 450/7915 â”ƒ Loss: 3.6571\n",
      "  Epoch 3/10 â”ƒ Batch 500/7915 â”ƒ Loss: 3.7110\n",
      "  Epoch 3/10 â”ƒ Batch 550/7915 â”ƒ Loss: 3.5004\n",
      "  Epoch 3/10 â”ƒ Batch 600/7915 â”ƒ Loss: 3.9142\n",
      "  Epoch 3/10 â”ƒ Batch 650/7915 â”ƒ Loss: 3.8672\n",
      "  Epoch 3/10 â”ƒ Batch 700/7915 â”ƒ Loss: 3.7039\n",
      "  Epoch 3/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.8894\n",
      "  Epoch 3/10 â”ƒ Batch 800/7915 â”ƒ Loss: 3.8761\n",
      "  Epoch 3/10 â”ƒ Batch 850/7915 â”ƒ Loss: 3.8346\n",
      "  Epoch 3/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.7642\n",
      "  Epoch 3/10 â”ƒ Batch 950/7915 â”ƒ Loss: 3.9482\n",
      "  Epoch 3/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 3.9528\n",
      "  Epoch 3/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 3.8592\n",
      "  Epoch 3/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 3.7906\n",
      "  Epoch 3/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 3.6877\n",
      "  Epoch 3/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.8215\n",
      "  Epoch 3/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.8648\n",
      "  Epoch 3/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 3.8702\n",
      "  Epoch 3/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 3.9156\n",
      "  Epoch 3/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 4.0404\n",
      "  Epoch 3/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.9201\n",
      "  Epoch 3/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.6111\n",
      "  Epoch 3/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 3.9087\n",
      "  Epoch 3/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 4.2871\n",
      "  Epoch 3/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.8498\n",
      "  Epoch 3/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 3.9505\n",
      "  Epoch 3/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 3.9513\n",
      "  Epoch 3/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.9947\n",
      "  Epoch 3/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 4.0199\n",
      "  Epoch 3/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 3.8494\n",
      "  Epoch 3/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 3.7632\n",
      "  Epoch 3/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 4.1709\n",
      "  Epoch 3/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 3.9866\n",
      "  Epoch 3/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 3.8299\n",
      "  Epoch 3/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 3.7119\n",
      "  Epoch 3/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 3.7104\n",
      "  Epoch 3/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 3.9529\n",
      "  Epoch 3/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.6537\n",
      "  Epoch 3/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.8021\n",
      "  Epoch 3/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.7195\n",
      "  Epoch 3/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.8096\n",
      "  Epoch 3/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.8522\n",
      "  Epoch 3/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 3.8848\n",
      "  Epoch 3/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 3.8015\n",
      "  Epoch 3/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 3.9855\n",
      "  Epoch 3/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 3.9346\n",
      "  Epoch 3/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 4.0086\n",
      "  Epoch 3/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 3.4140\n",
      "  Epoch 3/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 3.6064\n",
      "  Epoch 3/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 3.8390\n",
      "  Epoch 3/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 3.7874\n",
      "  Epoch 3/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 3.9001\n",
      "  Epoch 3/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.8665\n",
      "  Epoch 3/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 3.8144\n",
      "  Epoch 3/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 3.8280\n",
      "  Epoch 3/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.8459\n",
      "  Epoch 3/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.8087\n",
      "  Epoch 3/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 3.7643\n",
      "  Epoch 3/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 3.5728\n",
      "  Epoch 3/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 3.2037\n",
      "  Epoch 3/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 3.8877\n",
      "  Epoch 3/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 3.9167\n",
      "  Epoch 3/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 3.7988\n",
      "  Epoch 3/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 4.0243\n",
      "  Epoch 3/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 3.8562\n",
      "  Epoch 3/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 3.6458\n",
      "  Epoch 3/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 4.0159\n",
      "  Epoch 3/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 3.4243\n",
      "  Epoch 3/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 3.6746\n",
      "  Epoch 3/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.6802\n",
      "  Epoch 3/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.7610\n",
      "  Epoch 3/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.9484\n",
      "  Epoch 3/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 3.8035\n",
      "  Epoch 3/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 3.8922\n",
      "  Epoch 3/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 3.6177\n",
      "  Epoch 3/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 3.8359\n",
      "  Epoch 3/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.5715\n",
      "  Epoch 3/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.8663\n",
      "  Epoch 3/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 4.0154\n",
      "  Epoch 3/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 3.6793\n",
      "  Epoch 3/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 3.9183\n",
      "  Epoch 3/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 3.7036\n",
      "  Epoch 3/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.7885\n",
      "  Epoch 3/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.8729\n",
      "  Epoch 3/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.7778\n",
      "  Epoch 3/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 4.0206\n",
      "  Epoch 3/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 3.6423\n",
      "  Epoch 3/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 3.5274\n",
      "  Epoch 3/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 3.6503\n",
      "  Epoch 3/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 3.6895\n",
      "  Epoch 3/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 4.0702\n",
      "  Epoch 3/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 4.0883\n",
      "  Epoch 3/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 3.8838\n",
      "  Epoch 3/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.6523\n",
      "  Epoch 3/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 4.0215\n",
      "  Epoch 3/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 3.9528\n",
      "  Epoch 3/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.6027\n",
      "  Epoch 3/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 4.0213\n",
      "  Epoch 3/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 3.9654\n",
      "  Epoch 3/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.8844\n",
      "  Epoch 3/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.7300\n",
      "  Epoch 3/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 4.0535\n",
      "  Epoch 3/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.8627\n",
      "  Epoch 3/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 3.7812\n",
      "  Epoch 3/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 4.2150\n",
      "  Epoch 3/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 3.8886\n",
      "  Epoch 3/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 3.8569\n",
      "  Epoch 3/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 3.7533\n",
      "  Epoch 3/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 4.3704\n",
      "  Epoch 3/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.6084\n",
      "  Epoch 3/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 3.7770\n",
      "  Epoch 3/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 3.5918\n",
      "  Epoch 3/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 3.9119\n",
      "  Epoch 3/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.3599\n",
      "  Epoch 3/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 3.8321\n",
      "  Epoch 3/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.6340\n",
      "  Epoch 3/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 4.0355\n",
      "  Epoch 3/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 3.5873\n",
      "  Epoch 3/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.8725\n",
      "  Epoch 3/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 3.9010\n",
      "  Epoch 3/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 4.0686\n",
      "  Epoch 3/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 3.7224\n",
      "  Epoch 3/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 3.2343\n",
      "  Epoch 3/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 3.6172\n",
      "  Epoch 3/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.4824\n",
      "  Epoch 3/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 3.8188\n",
      "  Epoch 3/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 3.9908\n",
      "  Epoch 3/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 3.8254\n",
      "  Epoch 3/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 3.7741\n",
      "  Epoch 3/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 3.7628\n",
      "  Epoch 3/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.3125\n",
      "  Epoch 3/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 3.2495\n",
      "  Epoch 3/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 3.8355\n",
      "  Epoch 3/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 3.9373\n",
      "  Epoch 3/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.9137\n",
      "  Epoch 3/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 3.8347\n",
      "  Epoch 3/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 3.7183\n",
      "  Epoch 3/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 3.8545\n",
      "  Epoch 3/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 3.6737\n",
      "  Epoch 3/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 4.1558\n",
      "  Epoch 3/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 3.2106\n",
      "  Epoch 3/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 3.9354\n",
      "  Epoch 3/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 3.8973\n",
      "  Epoch 3/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 3.8984\n",
      "  Epoch 3/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 3.5220\n",
      "  Epoch 3/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 3.7655\n",
      "  Epoch 3/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 3.9523\n",
      "  Epoch 3/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.6571\n",
      "  Epoch 3/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 3.7029\n",
      "  Epoch 3/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 3.9832\n",
      "  Epoch 3/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.6778\n",
      "Epoch 3: Train Loss = 30.6031, Val Acc = 0.0513, Val Loss = 3.7897\n",
      "  Epoch 4/10 â”ƒ Batch 50/7915 â”ƒ Loss: 3.8859\n",
      "  Epoch 4/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.8783\n",
      "  Epoch 4/10 â”ƒ Batch 150/7915 â”ƒ Loss: 3.7314\n",
      "  Epoch 4/10 â”ƒ Batch 200/7915 â”ƒ Loss: 3.2906\n",
      "  Epoch 4/10 â”ƒ Batch 250/7915 â”ƒ Loss: 4.0856\n",
      "  Epoch 4/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.4759\n",
      "  Epoch 4/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.4442\n",
      "  Epoch 4/10 â”ƒ Batch 400/7915 â”ƒ Loss: 3.8130\n",
      "  Epoch 4/10 â”ƒ Batch 450/7915 â”ƒ Loss: 3.8119\n",
      "  Epoch 4/10 â”ƒ Batch 500/7915 â”ƒ Loss: 3.3247\n",
      "  Epoch 4/10 â”ƒ Batch 550/7915 â”ƒ Loss: 3.9773\n",
      "  Epoch 4/10 â”ƒ Batch 600/7915 â”ƒ Loss: 3.8816\n",
      "  Epoch 4/10 â”ƒ Batch 650/7915 â”ƒ Loss: 3.9308\n",
      "  Epoch 4/10 â”ƒ Batch 700/7915 â”ƒ Loss: 3.5898\n",
      "  Epoch 4/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.7328\n",
      "  Epoch 4/10 â”ƒ Batch 800/7915 â”ƒ Loss: 4.0598\n",
      "  Epoch 4/10 â”ƒ Batch 850/7915 â”ƒ Loss: 3.7956\n",
      "  Epoch 4/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.8833\n",
      "  Epoch 4/10 â”ƒ Batch 950/7915 â”ƒ Loss: 4.0735\n",
      "  Epoch 4/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 4.1825\n",
      "  Epoch 4/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 4.0149\n",
      "  Epoch 4/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 3.8951\n",
      "  Epoch 4/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 3.6855\n",
      "  Epoch 4/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.6601\n",
      "  Epoch 4/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.8609\n",
      "  Epoch 4/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 4.1677\n",
      "  Epoch 4/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 4.0792\n",
      "  Epoch 4/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 3.7651\n",
      "  Epoch 4/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.5510\n",
      "  Epoch 4/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.0965\n",
      "  Epoch 4/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 4.0974\n",
      "  Epoch 4/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 4.0068\n",
      "  Epoch 4/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.5801\n",
      "  Epoch 4/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 3.7917\n",
      "  Epoch 4/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 3.6229\n",
      "  Epoch 4/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.4140\n",
      "  Epoch 4/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 3.6525\n",
      "  Epoch 4/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 4.0185\n",
      "  Epoch 4/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 4.0741\n",
      "  Epoch 4/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 4.0198\n",
      "  Epoch 4/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 3.6173\n",
      "  Epoch 4/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 3.7171\n",
      "  Epoch 4/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 3.8222\n",
      "  Epoch 4/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 4.0490\n",
      "  Epoch 4/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 3.7294\n",
      "  Epoch 4/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.7445\n",
      "  Epoch 4/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.8624\n",
      "  Epoch 4/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.9384\n",
      "  Epoch 4/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.7428\n",
      "  Epoch 4/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.6786\n",
      "  Epoch 4/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 3.4834\n",
      "  Epoch 4/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 3.8275\n",
      "  Epoch 4/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 3.5908\n",
      "  Epoch 4/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 3.5877\n",
      "  Epoch 4/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 3.9168\n",
      "  Epoch 4/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 3.8736\n",
      "  Epoch 4/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 4.0400\n",
      "  Epoch 4/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 3.7934\n",
      "  Epoch 4/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 3.3965\n",
      "  Epoch 4/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 3.8011\n",
      "  Epoch 4/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.2564\n",
      "  Epoch 4/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 3.6797\n",
      "  Epoch 4/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 3.8420\n",
      "  Epoch 4/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.5534\n",
      "  Epoch 4/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.6860\n",
      "  Epoch 4/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 3.5687\n",
      "  Epoch 4/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 3.7623\n",
      "  Epoch 4/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 3.5133\n",
      "  Epoch 4/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 3.7937\n",
      "  Epoch 4/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 4.0025\n",
      "  Epoch 4/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 3.2639\n",
      "  Epoch 4/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 3.7726\n",
      "  Epoch 4/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 3.5209\n",
      "  Epoch 4/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 3.9066\n",
      "  Epoch 4/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 3.0154\n",
      "  Epoch 4/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 3.2355\n",
      "  Epoch 4/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 3.8797\n",
      "  Epoch 4/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.6139\n",
      "  Epoch 4/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.6807\n",
      "  Epoch 4/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.5490\n",
      "  Epoch 4/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 3.5708\n",
      "  Epoch 4/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 3.6606\n",
      "  Epoch 4/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 3.2750\n",
      "  Epoch 4/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 3.5984\n",
      "  Epoch 4/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.8365\n",
      "  Epoch 4/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.7323\n",
      "  Epoch 4/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 3.9130\n",
      "  Epoch 4/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 3.5145\n",
      "  Epoch 4/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 3.7477\n",
      "  Epoch 4/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 3.1421\n",
      "  Epoch 4/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.9593\n",
      "  Epoch 4/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.7388\n",
      "  Epoch 4/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.9063\n",
      "  Epoch 4/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 3.2687\n",
      "  Epoch 4/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 3.2429\n",
      "  Epoch 4/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 3.8888\n",
      "  Epoch 4/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 3.6631\n",
      "  Epoch 4/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 3.7358\n",
      "  Epoch 4/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 3.9018\n",
      "  Epoch 4/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 4.3386\n",
      "  Epoch 4/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 3.0998\n",
      "  Epoch 4/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.1589\n",
      "  Epoch 4/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 3.5911\n",
      "  Epoch 4/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 4.0714\n",
      "  Epoch 4/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.5362\n",
      "  Epoch 4/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 3.6340\n",
      "  Epoch 4/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 3.7278\n",
      "  Epoch 4/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.5109\n",
      "  Epoch 4/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.8365\n",
      "  Epoch 4/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 3.9037\n",
      "  Epoch 4/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.5030\n",
      "  Epoch 4/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 4.1527\n",
      "  Epoch 4/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 3.9150\n",
      "  Epoch 4/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 3.6391\n",
      "  Epoch 4/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 3.7531\n",
      "  Epoch 4/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 3.8625\n",
      "  Epoch 4/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 3.9353\n",
      "  Epoch 4/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.8470\n",
      "  Epoch 4/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 4.1117\n",
      "  Epoch 4/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 3.7730\n",
      "  Epoch 4/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 3.8552\n",
      "  Epoch 4/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.8052\n",
      "  Epoch 4/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 3.2225\n",
      "  Epoch 4/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.9092\n",
      "  Epoch 4/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 3.7473\n",
      "  Epoch 4/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 4.0183\n",
      "  Epoch 4/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.9683\n",
      "  Epoch 4/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 3.7431\n",
      "  Epoch 4/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 4.2204\n",
      "  Epoch 4/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 3.7985\n",
      "  Epoch 4/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 3.9183\n",
      "  Epoch 4/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 3.6440\n",
      "  Epoch 4/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.7670\n",
      "  Epoch 4/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 3.7767\n",
      "  Epoch 4/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 3.6317\n",
      "  Epoch 4/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 3.8509\n",
      "  Epoch 4/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 3.9813\n",
      "  Epoch 4/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 4.1446\n",
      "  Epoch 4/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.9371\n",
      "  Epoch 4/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 3.9507\n",
      "  Epoch 4/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 3.7872\n",
      "  Epoch 4/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 3.8281\n",
      "  Epoch 4/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.9768\n",
      "  Epoch 4/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 3.8878\n",
      "  Epoch 4/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 4.0749\n",
      "  Epoch 4/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 3.7695\n",
      "  Epoch 4/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 3.7112\n",
      "  Epoch 4/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 4.2836\n",
      "  Epoch 4/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 3.8385\n",
      "  Epoch 4/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 3.5478\n",
      "  Epoch 4/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 3.8507\n",
      "  Epoch 4/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 2.5303\n",
      "  Epoch 4/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 3.9639\n",
      "  Epoch 4/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 3.6786\n",
      "  Epoch 4/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 4.0055\n",
      "  Epoch 4/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.9367\n",
      "  Epoch 4/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 3.8309\n",
      "  Epoch 4/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 3.5773\n",
      "  Epoch 4/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.8759\n",
      "Epoch 4: Train Loss = 30.1810, Val Acc = 0.0625, Val Loss = 3.7620\n",
      "  Epoch 5/10 â”ƒ Batch 50/7915 â”ƒ Loss: 3.8908\n",
      "  Epoch 5/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.7632\n",
      "  Epoch 5/10 â”ƒ Batch 150/7915 â”ƒ Loss: 4.1557\n",
      "  Epoch 5/10 â”ƒ Batch 200/7915 â”ƒ Loss: 3.6538\n",
      "  Epoch 5/10 â”ƒ Batch 250/7915 â”ƒ Loss: 3.9790\n",
      "  Epoch 5/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.5040\n",
      "  Epoch 5/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.6605\n",
      "  Epoch 5/10 â”ƒ Batch 400/7915 â”ƒ Loss: 3.5008\n",
      "  Epoch 5/10 â”ƒ Batch 450/7915 â”ƒ Loss: 4.1083\n",
      "  Epoch 5/10 â”ƒ Batch 500/7915 â”ƒ Loss: 3.6651\n",
      "  Epoch 5/10 â”ƒ Batch 550/7915 â”ƒ Loss: 4.1162\n",
      "  Epoch 5/10 â”ƒ Batch 600/7915 â”ƒ Loss: 3.9018\n",
      "  Epoch 5/10 â”ƒ Batch 650/7915 â”ƒ Loss: 3.7262\n",
      "  Epoch 5/10 â”ƒ Batch 700/7915 â”ƒ Loss: 3.5399\n",
      "  Epoch 5/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.1485\n",
      "  Epoch 5/10 â”ƒ Batch 800/7915 â”ƒ Loss: 3.8142\n",
      "  Epoch 5/10 â”ƒ Batch 850/7915 â”ƒ Loss: 3.8875\n",
      "  Epoch 5/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.7629\n",
      "  Epoch 5/10 â”ƒ Batch 950/7915 â”ƒ Loss: 3.2746\n",
      "  Epoch 5/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 3.6229\n",
      "  Epoch 5/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 3.3429\n",
      "  Epoch 5/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 3.4143\n",
      "  Epoch 5/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 3.5742\n",
      "  Epoch 5/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.7537\n",
      "  Epoch 5/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.6681\n",
      "  Epoch 5/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 3.8576\n",
      "  Epoch 5/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 3.7403\n",
      "  Epoch 5/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 4.0681\n",
      "  Epoch 5/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.5862\n",
      "  Epoch 5/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.8886\n",
      "  Epoch 5/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 3.6650\n",
      "  Epoch 5/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 3.7210\n",
      "  Epoch 5/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.2270\n",
      "  Epoch 5/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 4.3738\n",
      "  Epoch 5/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 3.6230\n",
      "  Epoch 5/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.5214\n",
      "  Epoch 5/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 3.8258\n",
      "  Epoch 5/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 3.3900\n",
      "  Epoch 5/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 3.6191\n",
      "  Epoch 5/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 3.8454\n",
      "  Epoch 5/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 3.6169\n",
      "  Epoch 5/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 3.8100\n",
      "  Epoch 5/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 3.6412\n",
      "  Epoch 5/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 3.8913\n",
      "  Epoch 5/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 4.1115\n",
      "  Epoch 5/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.4687\n",
      "  Epoch 5/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.7897\n",
      "  Epoch 5/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.4372\n",
      "  Epoch 5/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.4824\n",
      "  Epoch 5/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.9006\n",
      "  Epoch 5/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 3.6685\n",
      "  Epoch 5/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 3.4515\n",
      "  Epoch 5/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 4.3127\n",
      "  Epoch 5/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 3.7252\n",
      "  Epoch 5/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 3.6031\n",
      "  Epoch 5/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 4.0203\n",
      "  Epoch 5/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 3.4350\n",
      "  Epoch 5/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 4.1356\n",
      "  Epoch 5/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 3.6273\n",
      "  Epoch 5/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 3.5815\n",
      "  Epoch 5/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.9106\n",
      "  Epoch 5/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 3.8854\n",
      "  Epoch 5/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 3.3971\n",
      "  Epoch 5/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.9194\n",
      "  Epoch 5/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.7172\n",
      "  Epoch 5/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 3.7526\n",
      "  Epoch 5/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 3.6001\n",
      "  Epoch 5/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 3.8827\n",
      "  Epoch 5/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 3.5384\n",
      "  Epoch 5/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 3.9190\n",
      "  Epoch 5/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 3.7315\n",
      "  Epoch 5/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 2.8357\n",
      "  Epoch 5/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 3.4676\n",
      "  Epoch 5/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 4.1229\n",
      "  Epoch 5/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 3.7056\n",
      "  Epoch 5/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 4.1188\n",
      "  Epoch 5/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 4.0292\n",
      "  Epoch 5/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.9179\n",
      "  Epoch 5/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.6280\n",
      "  Epoch 5/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.7368\n",
      "  Epoch 5/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 4.0541\n",
      "  Epoch 5/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 3.9663\n",
      "  Epoch 5/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 3.4656\n",
      "  Epoch 5/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 3.8221\n",
      "  Epoch 5/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.7815\n",
      "  Epoch 5/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.9655\n",
      "  Epoch 5/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 3.8537\n",
      "  Epoch 5/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 3.5882\n",
      "  Epoch 5/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 4.2320\n",
      "  Epoch 5/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 4.0255\n",
      "  Epoch 5/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.7400\n",
      "  Epoch 5/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.7352\n",
      "  Epoch 5/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.7797\n",
      "  Epoch 5/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 3.5794\n",
      "  Epoch 5/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 3.1161\n",
      "  Epoch 5/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 3.7058\n",
      "  Epoch 5/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 4.0526\n",
      "  Epoch 5/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 3.6418\n",
      "  Epoch 5/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 4.2142\n",
      "  Epoch 5/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 3.3817\n",
      "  Epoch 5/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 3.5688\n",
      "  Epoch 5/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.1951\n",
      "  Epoch 5/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 3.8690\n",
      "  Epoch 5/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 3.3973\n",
      "  Epoch 5/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.9398\n",
      "  Epoch 5/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 3.9248\n",
      "  Epoch 5/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 3.9344\n",
      "  Epoch 5/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.6994\n",
      "  Epoch 5/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.4597\n",
      "  Epoch 5/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 3.7125\n",
      "  Epoch 5/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.8756\n",
      "  Epoch 5/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 3.7231\n",
      "  Epoch 5/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 3.9424\n",
      "  Epoch 5/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 3.7769\n",
      "  Epoch 5/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 4.0356\n",
      "  Epoch 5/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 4.0191\n",
      "  Epoch 5/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 3.5235\n",
      "  Epoch 5/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.7654\n",
      "  Epoch 5/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 3.8445\n",
      "  Epoch 5/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 3.9124\n",
      "  Epoch 5/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 3.8761\n",
      "  Epoch 5/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.5912\n",
      "  Epoch 5/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 3.6080\n",
      "  Epoch 5/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.6417\n",
      "  Epoch 5/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 4.1168\n",
      "  Epoch 5/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 3.9331\n",
      "  Epoch 5/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.8577\n",
      "  Epoch 5/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 3.7589\n",
      "  Epoch 5/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 4.2822\n",
      "  Epoch 5/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 4.1258\n",
      "  Epoch 5/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 2.9962\n",
      "  Epoch 5/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 3.4187\n",
      "  Epoch 5/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.8232\n",
      "  Epoch 5/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 4.0419\n",
      "  Epoch 5/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 3.7647\n",
      "  Epoch 5/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 3.7983\n",
      "  Epoch 5/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 3.0979\n",
      "  Epoch 5/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 3.7994\n",
      "  Epoch 5/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.8421\n",
      "  Epoch 5/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 3.7436\n",
      "  Epoch 5/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 3.6860\n",
      "  Epoch 5/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 4.1165\n",
      "  Epoch 5/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.6741\n",
      "  Epoch 5/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 3.7568\n",
      "  Epoch 5/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 3.8482\n",
      "  Epoch 5/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 3.9503\n",
      "  Epoch 5/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 3.7819\n",
      "  Epoch 5/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 3.6254\n",
      "  Epoch 5/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 2.9933\n",
      "  Epoch 5/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 3.4013\n",
      "  Epoch 5/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 3.9903\n",
      "  Epoch 5/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 3.8050\n",
      "  Epoch 5/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 3.8252\n",
      "  Epoch 5/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 3.8449\n",
      "  Epoch 5/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 3.8882\n",
      "  Epoch 5/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.4852\n",
      "  Epoch 5/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 3.4371\n",
      "  Epoch 5/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 3.7663\n",
      "  Epoch 5/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.4549\n",
      "Epoch 5: Train Loss = 29.9476, Val Acc = 0.0737, Val Loss = 3.7334\n",
      "  Epoch 6/10 â”ƒ Batch 50/7915 â”ƒ Loss: 3.4365\n",
      "  Epoch 6/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.2195\n",
      "  Epoch 6/10 â”ƒ Batch 150/7915 â”ƒ Loss: 3.9302\n",
      "  Epoch 6/10 â”ƒ Batch 200/7915 â”ƒ Loss: 4.1129\n",
      "  Epoch 6/10 â”ƒ Batch 250/7915 â”ƒ Loss: 3.7585\n",
      "  Epoch 6/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.7677\n",
      "  Epoch 6/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.6951\n",
      "  Epoch 6/10 â”ƒ Batch 400/7915 â”ƒ Loss: 2.9757\n",
      "  Epoch 6/10 â”ƒ Batch 450/7915 â”ƒ Loss: 3.2798\n",
      "  Epoch 6/10 â”ƒ Batch 500/7915 â”ƒ Loss: 3.5115\n",
      "  Epoch 6/10 â”ƒ Batch 550/7915 â”ƒ Loss: 3.7034\n",
      "  Epoch 6/10 â”ƒ Batch 600/7915 â”ƒ Loss: 4.0410\n",
      "  Epoch 6/10 â”ƒ Batch 650/7915 â”ƒ Loss: 3.1321\n",
      "  Epoch 6/10 â”ƒ Batch 700/7915 â”ƒ Loss: 3.1478\n",
      "  Epoch 6/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.6564\n",
      "  Epoch 6/10 â”ƒ Batch 800/7915 â”ƒ Loss: 3.8915\n",
      "  Epoch 6/10 â”ƒ Batch 850/7915 â”ƒ Loss: 3.8492\n",
      "  Epoch 6/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.4511\n",
      "  Epoch 6/10 â”ƒ Batch 950/7915 â”ƒ Loss: 3.9375\n",
      "  Epoch 6/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 3.9076\n",
      "  Epoch 6/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 3.8064\n",
      "  Epoch 6/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 2.8972\n",
      "  Epoch 6/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 3.6878\n",
      "  Epoch 6/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.7562\n",
      "  Epoch 6/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.7663\n",
      "  Epoch 6/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 3.4734\n",
      "  Epoch 6/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 3.9169\n",
      "  Epoch 6/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 3.8464\n",
      "  Epoch 6/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.6306\n",
      "  Epoch 6/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.3796\n",
      "  Epoch 6/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 3.3039\n",
      "  Epoch 6/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 3.8997\n",
      "  Epoch 6/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.6478\n",
      "  Epoch 6/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 3.5428\n",
      "  Epoch 6/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 4.0696\n",
      "  Epoch 6/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.7760\n",
      "  Epoch 6/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 3.3659\n",
      "  Epoch 6/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 3.4424\n",
      "  Epoch 6/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 3.6831\n",
      "  Epoch 6/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 3.7703\n",
      "  Epoch 6/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 3.8209\n",
      "  Epoch 6/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 3.9025\n",
      "  Epoch 6/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 3.4334\n",
      "  Epoch 6/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 3.5322\n",
      "  Epoch 6/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 3.8222\n",
      "  Epoch 6/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.7877\n",
      "  Epoch 6/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.7292\n",
      "  Epoch 6/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.4835\n",
      "  Epoch 6/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.9964\n",
      "  Epoch 6/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.4879\n",
      "  Epoch 6/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 3.5626\n",
      "  Epoch 6/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 4.0698\n",
      "  Epoch 6/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 4.1375\n",
      "  Epoch 6/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 3.6769\n",
      "  Epoch 6/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 3.3753\n",
      "  Epoch 6/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 3.7785\n",
      "  Epoch 6/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 4.0196\n",
      "  Epoch 6/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 3.5115\n",
      "  Epoch 6/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 4.1957\n",
      "  Epoch 6/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 3.1915\n",
      "  Epoch 6/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.9327\n",
      "  Epoch 6/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 3.8281\n",
      "  Epoch 6/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 4.0337\n",
      "  Epoch 6/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.9006\n",
      "  Epoch 6/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.7719\n",
      "  Epoch 6/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 3.7337\n",
      "  Epoch 6/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 3.7074\n",
      "  Epoch 6/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 3.5721\n",
      "  Epoch 6/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 4.1972\n",
      "  Epoch 6/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 3.8826\n",
      "  Epoch 6/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 4.0396\n",
      "  Epoch 6/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 3.9242\n",
      "  Epoch 6/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 4.0371\n",
      "  Epoch 6/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 3.3488\n",
      "  Epoch 6/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 3.9622\n",
      "  Epoch 6/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 3.7897\n",
      "  Epoch 6/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 3.9221\n",
      "  Epoch 6/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.7417\n",
      "  Epoch 6/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.9196\n",
      "  Epoch 6/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.8096\n",
      "  Epoch 6/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 3.4906\n",
      "  Epoch 6/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 3.9587\n",
      "  Epoch 6/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 3.3774\n",
      "  Epoch 6/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 3.6704\n",
      "  Epoch 6/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.4083\n",
      "  Epoch 6/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.7574\n",
      "  Epoch 6/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 3.8102\n",
      "  Epoch 6/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 3.6634\n",
      "  Epoch 6/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 3.7230\n",
      "  Epoch 6/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 3.9449\n",
      "  Epoch 6/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.6744\n",
      "  Epoch 6/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.8700\n",
      "  Epoch 6/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.0460\n",
      "  Epoch 6/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 3.9285\n",
      "  Epoch 6/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 3.7777\n",
      "  Epoch 6/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 3.9349\n",
      "  Epoch 6/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 4.0201\n",
      "  Epoch 6/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 4.0772\n",
      "  Epoch 6/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 3.5281\n",
      "  Epoch 6/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 3.5379\n",
      "  Epoch 6/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 3.8777\n",
      "  Epoch 6/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.8646\n",
      "  Epoch 6/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 3.5657\n",
      "  Epoch 6/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 3.6719\n",
      "  Epoch 6/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.5745\n",
      "  Epoch 6/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 4.0522\n",
      "  Epoch 6/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 4.0412\n",
      "  Epoch 6/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.2522\n",
      "  Epoch 6/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.5878\n",
      "  Epoch 6/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 3.7309\n",
      "  Epoch 6/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.7489\n",
      "  Epoch 6/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 3.3643\n",
      "  Epoch 6/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 3.6069\n",
      "  Epoch 6/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 4.1557\n",
      "  Epoch 6/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 4.0685\n",
      "  Epoch 6/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 3.6448\n",
      "  Epoch 6/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 3.7289\n",
      "  Epoch 6/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.8594\n",
      "  Epoch 6/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 3.7117\n",
      "  Epoch 6/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 3.7962\n",
      "  Epoch 6/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 3.6732\n",
      "  Epoch 6/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.8960\n",
      "  Epoch 6/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 3.9239\n",
      "  Epoch 6/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.6029\n",
      "  Epoch 6/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 3.8996\n",
      "  Epoch 6/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 3.6846\n",
      "  Epoch 6/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.5591\n",
      "  Epoch 6/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 4.1595\n",
      "  Epoch 6/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 3.6947\n",
      "  Epoch 6/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 3.8867\n",
      "  Epoch 6/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 3.6900\n",
      "  Epoch 6/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 4.0263\n",
      "  Epoch 6/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.5765\n",
      "  Epoch 6/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 3.8269\n",
      "  Epoch 6/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 4.0360\n",
      "  Epoch 6/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 3.6175\n",
      "  Epoch 6/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 4.0621\n",
      "  Epoch 6/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 4.0654\n",
      "  Epoch 6/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.6298\n",
      "  Epoch 6/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 3.7682\n",
      "  Epoch 6/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 3.7567\n",
      "  Epoch 6/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 3.7823\n",
      "  Epoch 6/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.8979\n",
      "  Epoch 6/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 2.8955\n",
      "  Epoch 6/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 3.0018\n",
      "  Epoch 6/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 3.4565\n",
      "  Epoch 6/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 3.7534\n",
      "  Epoch 6/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 3.1009\n",
      "  Epoch 6/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 4.1863\n",
      "  Epoch 6/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 3.7481\n",
      "  Epoch 6/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 3.7098\n",
      "  Epoch 6/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 3.2584\n",
      "  Epoch 6/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 3.5246\n",
      "  Epoch 6/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 4.0149\n",
      "  Epoch 6/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 3.7663\n",
      "  Epoch 6/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.8296\n",
      "  Epoch 6/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 3.9090\n",
      "  Epoch 6/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 3.7789\n",
      "  Epoch 6/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.6883\n",
      "Epoch 6: Train Loss = 29.6987, Val Acc = 0.0625, Val Loss = 3.7152\n",
      "  Epoch 7/10 â”ƒ Batch 50/7915 â”ƒ Loss: 4.0754\n",
      "  Epoch 7/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.5996\n",
      "  Epoch 7/10 â”ƒ Batch 150/7915 â”ƒ Loss: 3.6531\n",
      "  Epoch 7/10 â”ƒ Batch 200/7915 â”ƒ Loss: 3.8903\n",
      "  Epoch 7/10 â”ƒ Batch 250/7915 â”ƒ Loss: 4.2035\n",
      "  Epoch 7/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.4393\n",
      "  Epoch 7/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.6687\n",
      "  Epoch 7/10 â”ƒ Batch 400/7915 â”ƒ Loss: 3.7619\n",
      "  Epoch 7/10 â”ƒ Batch 450/7915 â”ƒ Loss: 4.1717\n",
      "  Epoch 7/10 â”ƒ Batch 500/7915 â”ƒ Loss: 3.7480\n",
      "  Epoch 7/10 â”ƒ Batch 550/7915 â”ƒ Loss: 3.5121\n",
      "  Epoch 7/10 â”ƒ Batch 600/7915 â”ƒ Loss: 3.7815\n",
      "  Epoch 7/10 â”ƒ Batch 650/7915 â”ƒ Loss: 3.9119\n",
      "  Epoch 7/10 â”ƒ Batch 700/7915 â”ƒ Loss: 3.7173\n",
      "  Epoch 7/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.2008\n",
      "  Epoch 7/10 â”ƒ Batch 800/7915 â”ƒ Loss: 3.5444\n",
      "  Epoch 7/10 â”ƒ Batch 850/7915 â”ƒ Loss: 3.7338\n",
      "  Epoch 7/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.9224\n",
      "  Epoch 7/10 â”ƒ Batch 950/7915 â”ƒ Loss: 3.0119\n",
      "  Epoch 7/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 4.2676\n",
      "  Epoch 7/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 3.4460\n",
      "  Epoch 7/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 3.9307\n",
      "  Epoch 7/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 3.7767\n",
      "  Epoch 7/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.3514\n",
      "  Epoch 7/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.9643\n",
      "  Epoch 7/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 3.4893\n",
      "  Epoch 7/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 3.9274\n",
      "  Epoch 7/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 3.9570\n",
      "  Epoch 7/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.3997\n",
      "  Epoch 7/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.9333\n",
      "  Epoch 7/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 3.7856\n",
      "  Epoch 7/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 3.6644\n",
      "  Epoch 7/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.7517\n",
      "  Epoch 7/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 3.4041\n",
      "  Epoch 7/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 3.4245\n",
      "  Epoch 7/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.9129\n",
      "  Epoch 7/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 3.5391\n",
      "  Epoch 7/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 3.7875\n",
      "  Epoch 7/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 3.4844\n",
      "  Epoch 7/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 3.1493\n",
      "  Epoch 7/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 3.7412\n",
      "  Epoch 7/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 4.0191\n",
      "  Epoch 7/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 3.8957\n",
      "  Epoch 7/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 3.8706\n",
      "  Epoch 7/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 3.6418\n",
      "  Epoch 7/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.1395\n",
      "  Epoch 7/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.4673\n",
      "  Epoch 7/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.7499\n",
      "  Epoch 7/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.6837\n",
      "  Epoch 7/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.0606\n",
      "  Epoch 7/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 3.3873\n",
      "  Epoch 7/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 4.0398\n",
      "  Epoch 7/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 2.3760\n",
      "  Epoch 7/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 3.8007\n",
      "  Epoch 7/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 3.6944\n",
      "  Epoch 7/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 3.5336\n",
      "  Epoch 7/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 3.2184\n",
      "  Epoch 7/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 3.8763\n",
      "  Epoch 7/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 3.3183\n",
      "  Epoch 7/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 3.9183\n",
      "  Epoch 7/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.9717\n",
      "  Epoch 7/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 4.0039\n",
      "  Epoch 7/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 3.7233\n",
      "  Epoch 7/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.4404\n",
      "  Epoch 7/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.4172\n",
      "  Epoch 7/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 2.6502\n",
      "  Epoch 7/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 4.0474\n",
      "  Epoch 7/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 4.1325\n",
      "  Epoch 7/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 3.8231\n",
      "  Epoch 7/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 3.9487\n",
      "  Epoch 7/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 3.9683\n",
      "  Epoch 7/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 3.4509\n",
      "  Epoch 7/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 3.4752\n",
      "  Epoch 7/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 3.8580\n",
      "  Epoch 7/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 3.6149\n",
      "  Epoch 7/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 3.3242\n",
      "  Epoch 7/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 3.8258\n",
      "  Epoch 7/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.6801\n",
      "  Epoch 7/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.1755\n",
      "  Epoch 7/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.2692\n",
      "  Epoch 7/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 3.8510\n",
      "  Epoch 7/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 3.5735\n",
      "  Epoch 7/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 3.6607\n",
      "  Epoch 7/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 3.4966\n",
      "  Epoch 7/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.7644\n",
      "  Epoch 7/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.7959\n",
      "  Epoch 7/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 4.0263\n",
      "  Epoch 7/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 4.1182\n",
      "  Epoch 7/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 3.6825\n",
      "  Epoch 7/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 4.1718\n",
      "  Epoch 7/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.4634\n",
      "  Epoch 7/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.6412\n",
      "  Epoch 7/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.6863\n",
      "  Epoch 7/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 4.0601\n",
      "  Epoch 7/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 3.7502\n",
      "  Epoch 7/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 4.0150\n",
      "  Epoch 7/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 3.5195\n",
      "  Epoch 7/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 3.5210\n",
      "  Epoch 7/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 3.5787\n",
      "  Epoch 7/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 3.9218\n",
      "  Epoch 7/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 3.7227\n",
      "  Epoch 7/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.2203\n",
      "  Epoch 7/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 3.7595\n",
      "  Epoch 7/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 3.6697\n",
      "  Epoch 7/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.4748\n",
      "  Epoch 7/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 3.7501\n",
      "  Epoch 7/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 3.1798\n",
      "  Epoch 7/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.9193\n",
      "  Epoch 7/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.6406\n",
      "  Epoch 7/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 3.8979\n",
      "  Epoch 7/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.7910\n",
      "  Epoch 7/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 3.9950\n",
      "  Epoch 7/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 3.7576\n",
      "  Epoch 7/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 3.6736\n",
      "  Epoch 7/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 3.6239\n",
      "  Epoch 7/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 3.8057\n",
      "  Epoch 7/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 3.1746\n",
      "  Epoch 7/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.7204\n",
      "  Epoch 7/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 3.5467\n",
      "  Epoch 7/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 2.8182\n",
      "  Epoch 7/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 3.8438\n",
      "  Epoch 7/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.7009\n",
      "  Epoch 7/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 3.6069\n",
      "  Epoch 7/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.8036\n",
      "  Epoch 7/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 3.6071\n",
      "  Epoch 7/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 3.9304\n",
      "  Epoch 7/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.5754\n",
      "  Epoch 7/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 3.6438\n",
      "  Epoch 7/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 3.6857\n",
      "  Epoch 7/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 3.9505\n",
      "  Epoch 7/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 3.3301\n",
      "  Epoch 7/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 3.8624\n",
      "  Epoch 7/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.9746\n",
      "  Epoch 7/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 3.7323\n",
      "  Epoch 7/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 3.1999\n",
      "  Epoch 7/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 4.0048\n",
      "  Epoch 7/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 2.9807\n",
      "  Epoch 7/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 3.7485\n",
      "  Epoch 7/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.8372\n",
      "  Epoch 7/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 3.6390\n",
      "  Epoch 7/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 4.3465\n",
      "  Epoch 7/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 3.8573\n",
      "  Epoch 7/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.5526\n",
      "  Epoch 7/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 3.7062\n",
      "  Epoch 7/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 3.9210\n",
      "  Epoch 7/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 3.3915\n",
      "  Epoch 7/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 4.0841\n",
      "  Epoch 7/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 3.4813\n",
      "  Epoch 7/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 3.8031\n",
      "  Epoch 7/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 3.6978\n",
      "  Epoch 7/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 4.0008\n",
      "  Epoch 7/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 3.4012\n",
      "  Epoch 7/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 3.5226\n",
      "  Epoch 7/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 3.2888\n",
      "  Epoch 7/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 3.4349\n",
      "  Epoch 7/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.8548\n",
      "  Epoch 7/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 3.5815\n",
      "  Epoch 7/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 2.5748\n",
      "  Epoch 7/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.3787\n",
      "Epoch 7: Train Loss = 29.4577, Val Acc = 0.0692, Val Loss = 3.6861\n",
      "  Epoch 8/10 â”ƒ Batch 50/7915 â”ƒ Loss: 3.5131\n",
      "  Epoch 8/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.8727\n",
      "  Epoch 8/10 â”ƒ Batch 150/7915 â”ƒ Loss: 3.6459\n",
      "  Epoch 8/10 â”ƒ Batch 200/7915 â”ƒ Loss: 3.6712\n",
      "  Epoch 8/10 â”ƒ Batch 250/7915 â”ƒ Loss: 2.5889\n",
      "  Epoch 8/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.7905\n",
      "  Epoch 8/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.2846\n",
      "  Epoch 8/10 â”ƒ Batch 400/7915 â”ƒ Loss: 3.8279\n",
      "  Epoch 8/10 â”ƒ Batch 450/7915 â”ƒ Loss: 3.9315\n",
      "  Epoch 8/10 â”ƒ Batch 500/7915 â”ƒ Loss: 4.1635\n",
      "  Epoch 8/10 â”ƒ Batch 550/7915 â”ƒ Loss: 2.7707\n",
      "  Epoch 8/10 â”ƒ Batch 600/7915 â”ƒ Loss: 3.1354\n",
      "  Epoch 8/10 â”ƒ Batch 650/7915 â”ƒ Loss: 2.4373\n",
      "  Epoch 8/10 â”ƒ Batch 700/7915 â”ƒ Loss: 3.7780\n",
      "  Epoch 8/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.9269\n",
      "  Epoch 8/10 â”ƒ Batch 800/7915 â”ƒ Loss: 4.2201\n",
      "  Epoch 8/10 â”ƒ Batch 850/7915 â”ƒ Loss: 3.7383\n",
      "  Epoch 8/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.7405\n",
      "  Epoch 8/10 â”ƒ Batch 950/7915 â”ƒ Loss: 3.8910\n",
      "  Epoch 8/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 3.4888\n",
      "  Epoch 8/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 3.5553\n",
      "  Epoch 8/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 3.7066\n",
      "  Epoch 8/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 3.3625\n",
      "  Epoch 8/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.2687\n",
      "  Epoch 8/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.9484\n",
      "  Epoch 8/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 4.0705\n",
      "  Epoch 8/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 3.1181\n",
      "  Epoch 8/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 3.9621\n",
      "  Epoch 8/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.4379\n",
      "  Epoch 8/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.5441\n",
      "  Epoch 8/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 3.8315\n",
      "  Epoch 8/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 3.1528\n",
      "  Epoch 8/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.9040\n",
      "  Epoch 8/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 4.1592\n",
      "  Epoch 8/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 4.1360\n",
      "  Epoch 8/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.4269\n",
      "  Epoch 8/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 3.6178\n",
      "  Epoch 8/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 3.9529\n",
      "  Epoch 8/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 4.1439\n",
      "  Epoch 8/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 3.6132\n",
      "  Epoch 8/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 3.8435\n",
      "  Epoch 8/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 3.6616\n",
      "  Epoch 8/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 4.1869\n",
      "  Epoch 8/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 3.7406\n",
      "  Epoch 8/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 2.6619\n",
      "  Epoch 8/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.9560\n",
      "  Epoch 8/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.8155\n",
      "  Epoch 8/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.7062\n",
      "  Epoch 8/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.0257\n",
      "  Epoch 8/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.5879\n",
      "  Epoch 8/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 2.7168\n",
      "  Epoch 8/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 3.0455\n",
      "  Epoch 8/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 3.8825\n",
      "  Epoch 8/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 4.1843\n",
      "  Epoch 8/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 3.7644\n",
      "  Epoch 8/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 3.7507\n",
      "  Epoch 8/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 3.7175\n",
      "  Epoch 8/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 3.2557\n",
      "  Epoch 8/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 3.7243\n",
      "  Epoch 8/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 3.1839\n",
      "  Epoch 8/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.6237\n",
      "  Epoch 8/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 3.6579\n",
      "  Epoch 8/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 3.1002\n",
      "  Epoch 8/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.2193\n",
      "  Epoch 8/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.9003\n",
      "  Epoch 8/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 3.8261\n",
      "  Epoch 8/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 3.7357\n",
      "  Epoch 8/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 3.3408\n",
      "  Epoch 8/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 3.6125\n",
      "  Epoch 8/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 3.8176\n",
      "  Epoch 8/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 3.4901\n",
      "  Epoch 8/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 3.8077\n",
      "  Epoch 8/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 3.9566\n",
      "  Epoch 8/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 3.6434\n",
      "  Epoch 8/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 3.6049\n",
      "  Epoch 8/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 3.2419\n",
      "  Epoch 8/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 3.6653\n",
      "  Epoch 8/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.7211\n",
      "  Epoch 8/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.7667\n",
      "  Epoch 8/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.4109\n",
      "  Epoch 8/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 4.1323\n",
      "  Epoch 8/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 4.1761\n",
      "  Epoch 8/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 4.0200\n",
      "  Epoch 8/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 4.0824\n",
      "  Epoch 8/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.7356\n",
      "  Epoch 8/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.7840\n",
      "  Epoch 8/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 3.5037\n",
      "  Epoch 8/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 3.6639\n",
      "  Epoch 8/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 3.6808\n",
      "  Epoch 8/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 3.4665\n",
      "  Epoch 8/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.7216\n",
      "  Epoch 8/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.6901\n",
      "  Epoch 8/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.6572\n",
      "  Epoch 8/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 3.8909\n",
      "  Epoch 8/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 4.1988\n",
      "  Epoch 8/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 3.6078\n",
      "  Epoch 8/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 3.9162\n",
      "  Epoch 8/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 2.6927\n",
      "  Epoch 8/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 4.0424\n",
      "  Epoch 8/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 3.7895\n",
      "  Epoch 8/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 4.1280\n",
      "  Epoch 8/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.8578\n",
      "  Epoch 8/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 4.0689\n",
      "  Epoch 8/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 2.8026\n",
      "  Epoch 8/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.6597\n",
      "  Epoch 8/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 3.8788\n",
      "  Epoch 8/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 4.0970\n",
      "  Epoch 8/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.7159\n",
      "  Epoch 8/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.5131\n",
      "  Epoch 8/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 3.9080\n",
      "  Epoch 8/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.4583\n",
      "  Epoch 8/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 3.2820\n",
      "  Epoch 8/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 3.6242\n",
      "  Epoch 8/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 4.4432\n",
      "  Epoch 8/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 3.7606\n",
      "  Epoch 8/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 3.9156\n",
      "  Epoch 8/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 4.1698\n",
      "  Epoch 8/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.5242\n",
      "  Epoch 8/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 2.6383\n",
      "  Epoch 8/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 3.7738\n",
      "  Epoch 8/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 4.1308\n",
      "  Epoch 8/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.5057\n",
      "  Epoch 8/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 4.0253\n",
      "  Epoch 8/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.6584\n",
      "  Epoch 8/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 3.4869\n",
      "  Epoch 8/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 2.9631\n",
      "  Epoch 8/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.7992\n",
      "  Epoch 8/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 3.6243\n",
      "  Epoch 8/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 3.6842\n",
      "  Epoch 8/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 3.8035\n",
      "  Epoch 8/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 3.3915\n",
      "  Epoch 8/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 4.4210\n",
      "  Epoch 8/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.8843\n",
      "  Epoch 8/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 4.0119\n",
      "  Epoch 8/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 3.8697\n",
      "  Epoch 8/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 4.2071\n",
      "  Epoch 8/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 3.9631\n",
      "  Epoch 8/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 2.8949\n",
      "  Epoch 8/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.8091\n",
      "  Epoch 8/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 2.8492\n",
      "  Epoch 8/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 3.9033\n",
      "  Epoch 8/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 3.9520\n",
      "  Epoch 8/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.4616\n",
      "  Epoch 8/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 4.1028\n",
      "  Epoch 8/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 3.7924\n",
      "  Epoch 8/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 3.4430\n",
      "  Epoch 8/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 3.7590\n",
      "  Epoch 8/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 3.6263\n",
      "  Epoch 8/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 3.8321\n",
      "  Epoch 8/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 3.7186\n",
      "  Epoch 8/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 3.5637\n",
      "  Epoch 8/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 3.2899\n",
      "  Epoch 8/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 4.0782\n",
      "  Epoch 8/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 3.5726\n",
      "  Epoch 8/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 4.2852\n",
      "  Epoch 8/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.7574\n",
      "  Epoch 8/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 2.6959\n",
      "  Epoch 8/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 3.2350\n",
      "  Epoch 8/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.2945\n",
      "Epoch 8: Train Loss = 29.2132, Val Acc = 0.0692, Val Loss = 3.6651\n",
      "  Epoch 9/10 â”ƒ Batch 50/7915 â”ƒ Loss: 3.5686\n",
      "  Epoch 9/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.8106\n",
      "  Epoch 9/10 â”ƒ Batch 150/7915 â”ƒ Loss: 4.0247\n",
      "  Epoch 9/10 â”ƒ Batch 200/7915 â”ƒ Loss: 3.1947\n",
      "  Epoch 9/10 â”ƒ Batch 250/7915 â”ƒ Loss: 3.6280\n",
      "  Epoch 9/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.6516\n",
      "  Epoch 9/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.9174\n",
      "  Epoch 9/10 â”ƒ Batch 400/7915 â”ƒ Loss: 3.7796\n",
      "  Epoch 9/10 â”ƒ Batch 450/7915 â”ƒ Loss: 3.1493\n",
      "  Epoch 9/10 â”ƒ Batch 500/7915 â”ƒ Loss: 4.0010\n",
      "  Epoch 9/10 â”ƒ Batch 550/7915 â”ƒ Loss: 3.3201\n",
      "  Epoch 9/10 â”ƒ Batch 600/7915 â”ƒ Loss: 3.7312\n",
      "  Epoch 9/10 â”ƒ Batch 650/7915 â”ƒ Loss: 2.3613\n",
      "  Epoch 9/10 â”ƒ Batch 700/7915 â”ƒ Loss: 2.7909\n",
      "  Epoch 9/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.6066\n",
      "  Epoch 9/10 â”ƒ Batch 800/7915 â”ƒ Loss: 3.0026\n",
      "  Epoch 9/10 â”ƒ Batch 850/7915 â”ƒ Loss: 3.5535\n",
      "  Epoch 9/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.8247\n",
      "  Epoch 9/10 â”ƒ Batch 950/7915 â”ƒ Loss: 3.7203\n",
      "  Epoch 9/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 4.2303\n",
      "  Epoch 9/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 3.8549\n",
      "  Epoch 9/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 3.3803\n",
      "  Epoch 9/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 2.6000\n",
      "  Epoch 9/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.4587\n",
      "  Epoch 9/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.0248\n",
      "  Epoch 9/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 3.9617\n",
      "  Epoch 9/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 3.9316\n",
      "  Epoch 9/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 4.0596\n",
      "  Epoch 9/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.9473\n",
      "  Epoch 9/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.7121\n",
      "  Epoch 9/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 3.3394\n",
      "  Epoch 9/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 3.6902\n",
      "  Epoch 9/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.9707\n",
      "  Epoch 9/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 3.6511\n",
      "  Epoch 9/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 3.4460\n",
      "  Epoch 9/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.2236\n",
      "  Epoch 9/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 4.0415\n",
      "  Epoch 9/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 3.3662\n",
      "  Epoch 9/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 3.7136\n",
      "  Epoch 9/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 3.9266\n",
      "  Epoch 9/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 2.7013\n",
      "  Epoch 9/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 4.0901\n",
      "  Epoch 9/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 3.5074\n",
      "  Epoch 9/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 3.1763\n",
      "  Epoch 9/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 2.7308\n",
      "  Epoch 9/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.3822\n",
      "  Epoch 9/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.7902\n",
      "  Epoch 9/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.8727\n",
      "  Epoch 9/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.9284\n",
      "  Epoch 9/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.7110\n",
      "  Epoch 9/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 3.8854\n",
      "  Epoch 9/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 3.2822\n",
      "  Epoch 9/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 3.8323\n",
      "  Epoch 9/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 3.7364\n",
      "  Epoch 9/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 3.9275\n",
      "  Epoch 9/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 3.7999\n",
      "  Epoch 9/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 4.1410\n",
      "  Epoch 9/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 3.0352\n",
      "  Epoch 9/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 3.7237\n",
      "  Epoch 9/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 3.4738\n",
      "  Epoch 9/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.9881\n",
      "  Epoch 9/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 2.9942\n",
      "  Epoch 9/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 3.0130\n",
      "  Epoch 9/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.6801\n",
      "  Epoch 9/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.9288\n",
      "  Epoch 9/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 3.8594\n",
      "  Epoch 9/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 3.7779\n",
      "  Epoch 9/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 3.8002\n",
      "  Epoch 9/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 3.1094\n",
      "  Epoch 9/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 3.2354\n",
      "  Epoch 9/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 3.6855\n",
      "  Epoch 9/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 3.7786\n",
      "  Epoch 9/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 3.9049\n",
      "  Epoch 9/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 3.1270\n",
      "  Epoch 9/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 3.4037\n",
      "  Epoch 9/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 3.5405\n",
      "  Epoch 9/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 3.6996\n",
      "  Epoch 9/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.7166\n",
      "  Epoch 9/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.4681\n",
      "  Epoch 9/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.3402\n",
      "  Epoch 9/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 3.1852\n",
      "  Epoch 9/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 3.0200\n",
      "  Epoch 9/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 3.3412\n",
      "  Epoch 9/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 3.2683\n",
      "  Epoch 9/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.7656\n",
      "  Epoch 9/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.7614\n",
      "  Epoch 9/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 3.9751\n",
      "  Epoch 9/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 3.6894\n",
      "  Epoch 9/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 4.1839\n",
      "  Epoch 9/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 4.2126\n",
      "  Epoch 9/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.6120\n",
      "  Epoch 9/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.6703\n",
      "  Epoch 9/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.9688\n",
      "  Epoch 9/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 3.2508\n",
      "  Epoch 9/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 3.7314\n",
      "  Epoch 9/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 4.1098\n",
      "  Epoch 9/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 3.9651\n",
      "  Epoch 9/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 3.4891\n",
      "  Epoch 9/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 3.2675\n",
      "  Epoch 9/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 2.5033\n",
      "  Epoch 9/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 3.2743\n",
      "  Epoch 9/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.7078\n",
      "  Epoch 9/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 3.6999\n",
      "  Epoch 9/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 3.3920\n",
      "  Epoch 9/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.5674\n",
      "  Epoch 9/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 3.2829\n",
      "  Epoch 9/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 2.9061\n",
      "  Epoch 9/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.7021\n",
      "  Epoch 9/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.5724\n",
      "  Epoch 9/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 3.7375\n",
      "  Epoch 9/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.7517\n",
      "  Epoch 9/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 3.5330\n",
      "  Epoch 9/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 3.9041\n",
      "  Epoch 9/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 3.4199\n",
      "  Epoch 9/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 3.7070\n",
      "  Epoch 9/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 3.8165\n",
      "  Epoch 9/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 3.6050\n",
      "  Epoch 9/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.3049\n",
      "  Epoch 9/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 3.6399\n",
      "  Epoch 9/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 4.2035\n",
      "  Epoch 9/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 3.8839\n",
      "  Epoch 9/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.6971\n",
      "  Epoch 9/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 3.0651\n",
      "  Epoch 9/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.6995\n",
      "  Epoch 9/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 3.9865\n",
      "  Epoch 9/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 4.0016\n",
      "  Epoch 9/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.8893\n",
      "  Epoch 9/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 3.5909\n",
      "  Epoch 9/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 3.5814\n",
      "  Epoch 9/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 4.0527\n",
      "  Epoch 9/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 4.2841\n",
      "  Epoch 9/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 3.5568\n",
      "  Epoch 9/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.2212\n",
      "  Epoch 9/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 3.7096\n",
      "  Epoch 9/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 4.1378\n",
      "  Epoch 9/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 3.6000\n",
      "  Epoch 9/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 3.3809\n",
      "  Epoch 9/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 3.8254\n",
      "  Epoch 9/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.7177\n",
      "  Epoch 9/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 3.6446\n",
      "  Epoch 9/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 3.8943\n",
      "  Epoch 9/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 3.9503\n",
      "  Epoch 9/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.6960\n",
      "  Epoch 9/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 3.3677\n",
      "  Epoch 9/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 3.3547\n",
      "  Epoch 9/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 4.0089\n",
      "  Epoch 9/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 4.1265\n",
      "  Epoch 9/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 3.8584\n",
      "  Epoch 9/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 3.5429\n",
      "  Epoch 9/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 2.8939\n",
      "  Epoch 9/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 3.6034\n",
      "  Epoch 9/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 3.5201\n",
      "  Epoch 9/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 3.6485\n",
      "  Epoch 9/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 3.5253\n",
      "  Epoch 9/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 3.6713\n",
      "  Epoch 9/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.9078\n",
      "  Epoch 9/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 4.0270\n",
      "  Epoch 9/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 3.9289\n",
      "  Epoch 9/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.8747\n",
      "Epoch 9: Train Loss = 28.9771, Val Acc = 0.0759, Val Loss = 3.6210\n",
      "  Epoch 10/10 â”ƒ Batch 50/7915 â”ƒ Loss: 3.5438\n",
      "  Epoch 10/10 â”ƒ Batch 100/7915 â”ƒ Loss: 3.1966\n",
      "  Epoch 10/10 â”ƒ Batch 150/7915 â”ƒ Loss: 3.5482\n",
      "  Epoch 10/10 â”ƒ Batch 200/7915 â”ƒ Loss: 3.6869\n",
      "  Epoch 10/10 â”ƒ Batch 250/7915 â”ƒ Loss: 3.3142\n",
      "  Epoch 10/10 â”ƒ Batch 300/7915 â”ƒ Loss: 3.4690\n",
      "  Epoch 10/10 â”ƒ Batch 350/7915 â”ƒ Loss: 3.8412\n",
      "  Epoch 10/10 â”ƒ Batch 400/7915 â”ƒ Loss: 3.3504\n",
      "  Epoch 10/10 â”ƒ Batch 450/7915 â”ƒ Loss: 3.8907\n",
      "  Epoch 10/10 â”ƒ Batch 500/7915 â”ƒ Loss: 3.4441\n",
      "  Epoch 10/10 â”ƒ Batch 550/7915 â”ƒ Loss: 3.3288\n",
      "  Epoch 10/10 â”ƒ Batch 600/7915 â”ƒ Loss: 3.4876\n",
      "  Epoch 10/10 â”ƒ Batch 650/7915 â”ƒ Loss: 3.4428\n",
      "  Epoch 10/10 â”ƒ Batch 700/7915 â”ƒ Loss: 4.1674\n",
      "  Epoch 10/10 â”ƒ Batch 750/7915 â”ƒ Loss: 3.3408\n",
      "  Epoch 10/10 â”ƒ Batch 800/7915 â”ƒ Loss: 3.3637\n",
      "  Epoch 10/10 â”ƒ Batch 850/7915 â”ƒ Loss: 4.0271\n",
      "  Epoch 10/10 â”ƒ Batch 900/7915 â”ƒ Loss: 3.4347\n",
      "  Epoch 10/10 â”ƒ Batch 950/7915 â”ƒ Loss: 3.9683\n",
      "  Epoch 10/10 â”ƒ Batch 1000/7915 â”ƒ Loss: 3.8587\n",
      "  Epoch 10/10 â”ƒ Batch 1050/7915 â”ƒ Loss: 3.5133\n",
      "  Epoch 10/10 â”ƒ Batch 1100/7915 â”ƒ Loss: 4.0854\n",
      "  Epoch 10/10 â”ƒ Batch 1150/7915 â”ƒ Loss: 3.7774\n",
      "  Epoch 10/10 â”ƒ Batch 1200/7915 â”ƒ Loss: 3.6742\n",
      "  Epoch 10/10 â”ƒ Batch 1250/7915 â”ƒ Loss: 3.5471\n",
      "  Epoch 10/10 â”ƒ Batch 1300/7915 â”ƒ Loss: 3.6268\n",
      "  Epoch 10/10 â”ƒ Batch 1350/7915 â”ƒ Loss: 4.1353\n",
      "  Epoch 10/10 â”ƒ Batch 1400/7915 â”ƒ Loss: 3.8167\n",
      "  Epoch 10/10 â”ƒ Batch 1450/7915 â”ƒ Loss: 3.8036\n",
      "  Epoch 10/10 â”ƒ Batch 1500/7915 â”ƒ Loss: 3.2456\n",
      "  Epoch 10/10 â”ƒ Batch 1550/7915 â”ƒ Loss: 3.3328\n",
      "  Epoch 10/10 â”ƒ Batch 1600/7915 â”ƒ Loss: 2.9132\n",
      "  Epoch 10/10 â”ƒ Batch 1650/7915 â”ƒ Loss: 3.8771\n",
      "  Epoch 10/10 â”ƒ Batch 1700/7915 â”ƒ Loss: 3.6265\n",
      "  Epoch 10/10 â”ƒ Batch 1750/7915 â”ƒ Loss: 3.5633\n",
      "  Epoch 10/10 â”ƒ Batch 1800/7915 â”ƒ Loss: 3.3468\n",
      "  Epoch 10/10 â”ƒ Batch 1850/7915 â”ƒ Loss: 3.7403\n",
      "  Epoch 10/10 â”ƒ Batch 1900/7915 â”ƒ Loss: 3.7189\n",
      "  Epoch 10/10 â”ƒ Batch 1950/7915 â”ƒ Loss: 3.5221\n",
      "  Epoch 10/10 â”ƒ Batch 2000/7915 â”ƒ Loss: 3.0068\n",
      "  Epoch 10/10 â”ƒ Batch 2050/7915 â”ƒ Loss: 3.5533\n",
      "  Epoch 10/10 â”ƒ Batch 2100/7915 â”ƒ Loss: 4.0364\n",
      "  Epoch 10/10 â”ƒ Batch 2150/7915 â”ƒ Loss: 3.9292\n",
      "  Epoch 10/10 â”ƒ Batch 2200/7915 â”ƒ Loss: 3.5118\n",
      "  Epoch 10/10 â”ƒ Batch 2250/7915 â”ƒ Loss: 3.8020\n",
      "  Epoch 10/10 â”ƒ Batch 2300/7915 â”ƒ Loss: 3.2037\n",
      "  Epoch 10/10 â”ƒ Batch 2350/7915 â”ƒ Loss: 3.6273\n",
      "  Epoch 10/10 â”ƒ Batch 2400/7915 â”ƒ Loss: 3.1744\n",
      "  Epoch 10/10 â”ƒ Batch 2450/7915 â”ƒ Loss: 3.7621\n",
      "  Epoch 10/10 â”ƒ Batch 2500/7915 â”ƒ Loss: 3.1264\n",
      "  Epoch 10/10 â”ƒ Batch 2550/7915 â”ƒ Loss: 3.6440\n",
      "  Epoch 10/10 â”ƒ Batch 2600/7915 â”ƒ Loss: 3.7661\n",
      "  Epoch 10/10 â”ƒ Batch 2650/7915 â”ƒ Loss: 3.7264\n",
      "  Epoch 10/10 â”ƒ Batch 2700/7915 â”ƒ Loss: 3.5221\n",
      "  Epoch 10/10 â”ƒ Batch 2750/7915 â”ƒ Loss: 3.5799\n",
      "  Epoch 10/10 â”ƒ Batch 2800/7915 â”ƒ Loss: 3.7834\n",
      "  Epoch 10/10 â”ƒ Batch 2850/7915 â”ƒ Loss: 3.7542\n",
      "  Epoch 10/10 â”ƒ Batch 2900/7915 â”ƒ Loss: 3.5409\n",
      "  Epoch 10/10 â”ƒ Batch 2950/7915 â”ƒ Loss: 3.2189\n",
      "  Epoch 10/10 â”ƒ Batch 3000/7915 â”ƒ Loss: 4.0638\n",
      "  Epoch 10/10 â”ƒ Batch 3050/7915 â”ƒ Loss: 3.7070\n",
      "  Epoch 10/10 â”ƒ Batch 3100/7915 â”ƒ Loss: 3.9207\n",
      "  Epoch 10/10 â”ƒ Batch 3150/7915 â”ƒ Loss: 3.6264\n",
      "  Epoch 10/10 â”ƒ Batch 3200/7915 â”ƒ Loss: 3.4479\n",
      "  Epoch 10/10 â”ƒ Batch 3250/7915 â”ƒ Loss: 3.5499\n",
      "  Epoch 10/10 â”ƒ Batch 3300/7915 â”ƒ Loss: 3.0976\n",
      "  Epoch 10/10 â”ƒ Batch 3350/7915 â”ƒ Loss: 3.5090\n",
      "  Epoch 10/10 â”ƒ Batch 3400/7915 â”ƒ Loss: 3.4856\n",
      "  Epoch 10/10 â”ƒ Batch 3450/7915 â”ƒ Loss: 3.6865\n",
      "  Epoch 10/10 â”ƒ Batch 3500/7915 â”ƒ Loss: 3.7068\n",
      "  Epoch 10/10 â”ƒ Batch 3550/7915 â”ƒ Loss: 3.9031\n",
      "  Epoch 10/10 â”ƒ Batch 3600/7915 â”ƒ Loss: 3.8883\n",
      "  Epoch 10/10 â”ƒ Batch 3650/7915 â”ƒ Loss: 3.7529\n",
      "  Epoch 10/10 â”ƒ Batch 3700/7915 â”ƒ Loss: 3.6671\n",
      "  Epoch 10/10 â”ƒ Batch 3750/7915 â”ƒ Loss: 3.5984\n",
      "  Epoch 10/10 â”ƒ Batch 3800/7915 â”ƒ Loss: 3.6665\n",
      "  Epoch 10/10 â”ƒ Batch 3850/7915 â”ƒ Loss: 4.0071\n",
      "  Epoch 10/10 â”ƒ Batch 3900/7915 â”ƒ Loss: 3.5044\n",
      "  Epoch 10/10 â”ƒ Batch 3950/7915 â”ƒ Loss: 3.6378\n",
      "  Epoch 10/10 â”ƒ Batch 4000/7915 â”ƒ Loss: 3.9116\n",
      "  Epoch 10/10 â”ƒ Batch 4050/7915 â”ƒ Loss: 2.7937\n",
      "  Epoch 10/10 â”ƒ Batch 4100/7915 â”ƒ Loss: 3.3892\n",
      "  Epoch 10/10 â”ƒ Batch 4150/7915 â”ƒ Loss: 3.6802\n",
      "  Epoch 10/10 â”ƒ Batch 4200/7915 â”ƒ Loss: 3.3414\n",
      "  Epoch 10/10 â”ƒ Batch 4250/7915 â”ƒ Loss: 3.1065\n",
      "  Epoch 10/10 â”ƒ Batch 4300/7915 â”ƒ Loss: 3.4385\n",
      "  Epoch 10/10 â”ƒ Batch 4350/7915 â”ƒ Loss: 3.6515\n",
      "  Epoch 10/10 â”ƒ Batch 4400/7915 â”ƒ Loss: 3.4261\n",
      "  Epoch 10/10 â”ƒ Batch 4450/7915 â”ƒ Loss: 3.6770\n",
      "  Epoch 10/10 â”ƒ Batch 4500/7915 â”ƒ Loss: 3.5102\n",
      "  Epoch 10/10 â”ƒ Batch 4550/7915 â”ƒ Loss: 3.9780\n",
      "  Epoch 10/10 â”ƒ Batch 4600/7915 â”ƒ Loss: 3.8930\n",
      "  Epoch 10/10 â”ƒ Batch 4650/7915 â”ƒ Loss: 3.9936\n",
      "  Epoch 10/10 â”ƒ Batch 4700/7915 â”ƒ Loss: 3.3242\n",
      "  Epoch 10/10 â”ƒ Batch 4750/7915 â”ƒ Loss: 2.8859\n",
      "  Epoch 10/10 â”ƒ Batch 4800/7915 â”ƒ Loss: 3.7209\n",
      "  Epoch 10/10 â”ƒ Batch 4850/7915 â”ƒ Loss: 3.9297\n",
      "  Epoch 10/10 â”ƒ Batch 4900/7915 â”ƒ Loss: 2.9156\n",
      "  Epoch 10/10 â”ƒ Batch 4950/7915 â”ƒ Loss: 3.6876\n",
      "  Epoch 10/10 â”ƒ Batch 5000/7915 â”ƒ Loss: 3.7619\n",
      "  Epoch 10/10 â”ƒ Batch 5050/7915 â”ƒ Loss: 3.7353\n",
      "  Epoch 10/10 â”ƒ Batch 5100/7915 â”ƒ Loss: 3.5054\n",
      "  Epoch 10/10 â”ƒ Batch 5150/7915 â”ƒ Loss: 3.1682\n",
      "  Epoch 10/10 â”ƒ Batch 5200/7915 â”ƒ Loss: 4.1472\n",
      "  Epoch 10/10 â”ƒ Batch 5250/7915 â”ƒ Loss: 3.8737\n",
      "  Epoch 10/10 â”ƒ Batch 5300/7915 â”ƒ Loss: 3.5791\n",
      "  Epoch 10/10 â”ƒ Batch 5350/7915 â”ƒ Loss: 3.3261\n",
      "  Epoch 10/10 â”ƒ Batch 5400/7915 â”ƒ Loss: 3.0904\n",
      "  Epoch 10/10 â”ƒ Batch 5450/7915 â”ƒ Loss: 3.8120\n",
      "  Epoch 10/10 â”ƒ Batch 5500/7915 â”ƒ Loss: 3.6086\n",
      "  Epoch 10/10 â”ƒ Batch 5550/7915 â”ƒ Loss: 3.6883\n",
      "  Epoch 10/10 â”ƒ Batch 5600/7915 â”ƒ Loss: 3.1296\n",
      "  Epoch 10/10 â”ƒ Batch 5650/7915 â”ƒ Loss: 2.5742\n",
      "  Epoch 10/10 â”ƒ Batch 5700/7915 â”ƒ Loss: 2.9490\n",
      "  Epoch 10/10 â”ƒ Batch 5750/7915 â”ƒ Loss: 3.5956\n",
      "  Epoch 10/10 â”ƒ Batch 5800/7915 â”ƒ Loss: 3.2116\n",
      "  Epoch 10/10 â”ƒ Batch 5850/7915 â”ƒ Loss: 3.8088\n",
      "  Epoch 10/10 â”ƒ Batch 5900/7915 â”ƒ Loss: 3.6181\n",
      "  Epoch 10/10 â”ƒ Batch 5950/7915 â”ƒ Loss: 3.3005\n",
      "  Epoch 10/10 â”ƒ Batch 6000/7915 â”ƒ Loss: 3.7038\n",
      "  Epoch 10/10 â”ƒ Batch 6050/7915 â”ƒ Loss: 3.6853\n",
      "  Epoch 10/10 â”ƒ Batch 6100/7915 â”ƒ Loss: 3.6093\n",
      "  Epoch 10/10 â”ƒ Batch 6150/7915 â”ƒ Loss: 3.5541\n",
      "  Epoch 10/10 â”ƒ Batch 6200/7915 â”ƒ Loss: 3.3579\n",
      "  Epoch 10/10 â”ƒ Batch 6250/7915 â”ƒ Loss: 3.0846\n",
      "  Epoch 10/10 â”ƒ Batch 6300/7915 â”ƒ Loss: 3.7801\n",
      "  Epoch 10/10 â”ƒ Batch 6350/7915 â”ƒ Loss: 3.8963\n",
      "  Epoch 10/10 â”ƒ Batch 6400/7915 â”ƒ Loss: 3.2824\n",
      "  Epoch 10/10 â”ƒ Batch 6450/7915 â”ƒ Loss: 3.7572\n",
      "  Epoch 10/10 â”ƒ Batch 6500/7915 â”ƒ Loss: 3.9621\n",
      "  Epoch 10/10 â”ƒ Batch 6550/7915 â”ƒ Loss: 3.2924\n",
      "  Epoch 10/10 â”ƒ Batch 6600/7915 â”ƒ Loss: 3.3452\n",
      "  Epoch 10/10 â”ƒ Batch 6650/7915 â”ƒ Loss: 3.7786\n",
      "  Epoch 10/10 â”ƒ Batch 6700/7915 â”ƒ Loss: 3.0072\n",
      "  Epoch 10/10 â”ƒ Batch 6750/7915 â”ƒ Loss: 3.3051\n",
      "  Epoch 10/10 â”ƒ Batch 6800/7915 â”ƒ Loss: 3.7525\n",
      "  Epoch 10/10 â”ƒ Batch 6850/7915 â”ƒ Loss: 4.0109\n",
      "  Epoch 10/10 â”ƒ Batch 6900/7915 â”ƒ Loss: 3.0802\n",
      "  Epoch 10/10 â”ƒ Batch 6950/7915 â”ƒ Loss: 3.0257\n",
      "  Epoch 10/10 â”ƒ Batch 7000/7915 â”ƒ Loss: 3.8464\n",
      "  Epoch 10/10 â”ƒ Batch 7050/7915 â”ƒ Loss: 3.4830\n",
      "  Epoch 10/10 â”ƒ Batch 7100/7915 â”ƒ Loss: 3.8844\n",
      "  Epoch 10/10 â”ƒ Batch 7150/7915 â”ƒ Loss: 3.5432\n",
      "  Epoch 10/10 â”ƒ Batch 7200/7915 â”ƒ Loss: 3.9783\n",
      "  Epoch 10/10 â”ƒ Batch 7250/7915 â”ƒ Loss: 3.6813\n",
      "  Epoch 10/10 â”ƒ Batch 7300/7915 â”ƒ Loss: 3.7264\n",
      "  Epoch 10/10 â”ƒ Batch 7350/7915 â”ƒ Loss: 4.0502\n",
      "  Epoch 10/10 â”ƒ Batch 7400/7915 â”ƒ Loss: 3.9992\n",
      "  Epoch 10/10 â”ƒ Batch 7450/7915 â”ƒ Loss: 2.7800\n",
      "  Epoch 10/10 â”ƒ Batch 7500/7915 â”ƒ Loss: 3.4708\n",
      "  Epoch 10/10 â”ƒ Batch 7550/7915 â”ƒ Loss: 3.3612\n",
      "  Epoch 10/10 â”ƒ Batch 7600/7915 â”ƒ Loss: 3.7720\n",
      "  Epoch 10/10 â”ƒ Batch 7650/7915 â”ƒ Loss: 3.5567\n",
      "  Epoch 10/10 â”ƒ Batch 7700/7915 â”ƒ Loss: 3.8248\n",
      "  Epoch 10/10 â”ƒ Batch 7750/7915 â”ƒ Loss: 3.9312\n",
      "  Epoch 10/10 â”ƒ Batch 7800/7915 â”ƒ Loss: 3.4248\n",
      "  Epoch 10/10 â”ƒ Batch 7850/7915 â”ƒ Loss: 3.7151\n",
      "  Epoch 10/10 â”ƒ Batch 7900/7915 â”ƒ Loss: 3.9298\n",
      "  Epoch 10/10 â”ƒ Batch 7915/7915 â”ƒ Loss: 3.4837\n",
      "Epoch 10: Train Loss = 28.7065, Val Acc = 0.0692, Val Loss = 3.6148\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Experimental Group] Training with inception-based module\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m model_exp = train_model(train_exp_loader, val_exp_loader, \u001b[38;5;28;01mTrue\u001b[39;00m, lr=\u001b[32m0.000001\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m acc_exp, loss_exp = evaluate(model_exp, test_exp_loader, nn.CrossEntropyLoss(), \u001b[43mdevice\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# å¯¦é©—çµ„ç¸½å…±è€—æ™‚: 6:48:14\n",
    "print(\"\\n[Experimental Group] Training with inception-based module\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_exp = train_model(train_exp_loader, val_exp_loader, True, lr=0.000001)\n",
    "acc_exp, loss_exp = evaluate(model_exp, test_exp_loader, nn.CrossEntropyLoss(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰é¢å¿˜äº†è¨­ç½®deviceï¼Œæ‰€ä»¥è©•ä¼°å¦å¤–å¯«\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "acc_exp, loss_exp = evaluate(model_exp, test_exp_loader, nn.CrossEntropyLoss(), device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7546a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Test Results ===\n",
      "Control Group     â†’ Accuracy: 0.2444, Loss: 2.8422\n",
      "Experimental Groupâ†’ Accuracy: 0.0915, Loss: 3.5688\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Test Results ===\")\n",
    "print(f\"Control Group     â†’ Accuracy: {acc_ctrl:.4f}, Loss: {loss_ctrl:.4f}\")\n",
    "print(f\"Experimental Groupâ†’ Accuracy: {acc_exp:.4f}, Loss: {loss_exp:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "god",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
